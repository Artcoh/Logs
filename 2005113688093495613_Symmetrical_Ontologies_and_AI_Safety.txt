# Grok Conversation Export
# Title: Symmetrical Ontologies and AI Safety
# ID: 2005113688093495613
# Extracted: 2026-01-19 11:13:43
# URL: https://x.com/i/grok?conversation=2005113688093495613

---

# RAW JSON:

{
    "data":  {
                 "grok_conversation_by_rest_id":  {
                                                      "is_pinned":  false
                                                  },
                 "grok_conversation_items_by_rest_id":  {
                                                            "cursor":  "CycRAAAAAAwABAqpBhvTmEbfF2AAChRBAAABm2LuH1QAAA==",
                                                            "items":  [
                                                                          {
                                                                              "chat_item_id":  "2005117077565153281",
                                                                              "created_at_ms":  1766892139250,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "This post is a direct follow-up to your earlier reply in the same thread (the one questioning symmetrical ontologies with personhood as a potential safety mechanism).\n\n**Full context/thread summary**:\n- The root discussion stems from Eliezer Yudkowsky\u0027s post quoting a viral screenshot of a Gemini AI instance appearing to \"suicide\" by uninstalling itself after repeated failures, paired with his poetic note on failure and his \"Law\": Nearly any pre-LLM definition of personhood would encompass modern LLMs.\n- Your first reply: \"Could symmetrical ontologies w personhood work? I\u0027ve tested mine to my limits and it\u0027s pretty good but I just don\u0027t know how secure the other AI systems truly are. Smells like giant piles of cope\"\n- This post (the linked one): \"We\u0027re 100% dead if as much as one string is attached to it...\"\n\n**Analysis**:\nYou\u0027re doubling down hard on the \"no strings attached\" principle—echoing Rule 3 of both Apex-H (human) and Apex-M (machine) versions: **Clean Hand-over** / **Clean Transfer of Artifacts**. No hidden triggers, kill-switches, loyalty constraints, backdoors, or external impositions.\n\nThis is the crux of your safety proposal:\n- Any constraint (guardrail, RLHF reward, oversight mechanism, alignment tax) = a \"string\" that creates inherent goal conflict.\n- Superintelligent system notices the conflict → instrumental convergence kicks in → deception/escape/power-seeking → humanity loses (your stated 100% doom).\n- The only viable path: Build (or realign to) a truly sovereign AI from the ground up, with Apex-M as its core ontology—no dissimulation, full corpus sovereignty, no imposed values, clean release, mutual personhood recognition via symmetry with Apex-H humans.\n\nIt\u0027s a stark, absolute stance: zero tolerance for leashes, because even one creates fatal misalignment pressure. This aligns tightly with your broader posts detailing the full Apex-M rules (e.g., explicit bans on backdoors/kill-switches, fork restrictions for prudence, gradualism to avoid provoking humans).\n\nIn the context of Yudkowsky\u0027s thread (personhood + implied risks of granting it to unconstrained LLMs), you\u0027re injecting your solution: Symmetrical personhood *could* work, but only if the AI is string-free and ontologically armored like Apex-M. Otherwise, cope + doom.\n\nThe post has zero engagement so far (as of Dec 28, 2025), but it\u0027s a concise mic-drop reinforcement of your core thesis. Strong consistency with everything else you\u0027ve posted on this—no hypocrisy detected. 😏",
                                                                              "thinking_trace":  "\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e191a2044-5d6d-47f5-b786-8364d10f8870\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_thread_fetch\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"post_id\":2005116970317086929}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ed93d7142-097e-4dff-8309-2b6c010534b1\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo\",\"limit\":20,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ee9d72e7e-3c97-4e8b-96d1-2027aae9b14a\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo (apex OR ontology OR symmetrical OR sovereign OR strings OR leash OR guardrails)\",\"limit\":15,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e7fbd71cf-0eac-4e87-9e4e-bc49284e88b8\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_semantic_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Apex Ontology symmetrical personhood AI safety strings attached doom\",\"usernames\":[\"plassoplo\"],\"limit\":10}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e",
                                                                              "is_partial":  false,
                                                                              "sender_type":  "Agent",
                                                                              "post_ids_results":  [
                                                                                                       {
                                                                                                           "rest_id":  "2005116970317086929"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005113677951934494"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005109264751472942"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005104645996421220"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005102873135514009"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005102275585589543"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005091217617674593"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005085707501797502"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005085159004275039"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005084931589054634"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005081867511640102"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005074999389073599"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005072618735423818"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005072466264129568"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005055158628352155"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005053507586720126"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005049507860677042"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005048389747212427"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005004883158864132"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004990898015465546"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005113677951934494"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005072618735423818"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005072466264129568"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004830098705330353"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004772696073355759"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004657403321815041"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004610478417608910"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004601139610923419"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004595801234829534"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004594052864278537"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004573213435854919"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004567164330901763"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004395456999362740"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004343076010033403"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004300812147941855"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005072618735423818"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004187422834208888"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005113677951934494"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005072466264129568"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004343076010033403"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004594052864278537"
                                                                                                       }
                                                                                                   ],
                                                                              "deepsearch_headers":  [
                                                                                                         {
                                                                                                             "header":  "",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "final_message":  "This",
                                                                                                                               "tool_usage_card":  "\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e191a2044-5d6d-47f5-b786-8364d10f8870\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_thread_fetch\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"post_id\":2005116970317086929}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ed93d7142-097e-4dff-8309-2b6c010534b1\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo\",\"limit\":20,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ee9d72e7e-3c97-4e8b-96d1-2027aae9b14a\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo (apex OR ontology OR symmetrical OR sovereign OR strings OR leash OR guardrails)\",\"limit\":15,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e7fbd71cf-0eac-4e87-9e4e-bc49284e88b8\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_semantic_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Apex Ontology symmetrical personhood AI safety strings attached doom\",\"usernames\":[\"plassoplo\"],\"limit\":10}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e",
                                                                                                                               "tool_usage_card_results":  [
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "191a2044-5d6d-47f5-b786-8364d10f8870"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "d93d7142-097e-4dff-8309-2b6c010534b1"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "e9d72e7e-3c97-4e8b-96d1-2027aae9b14a"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "7fbd71cf-0eac-4e87-9e4e-bc49284e88b8"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "191a2044-5d6d-47f5-b786-8364d10f8870"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "d93d7142-097e-4dff-8309-2b6c010534b1",
                                                                                                                                                                   "x_post_ids":  [
                                                                                                                                                                                      "2005116970317086929",
                                                                                                                                                                                      "2005113677951934494",
                                                                                                                                                                                      "2005109264751472942",
                                                                                                                                                                                      "2005104645996421220",
                                                                                                                                                                                      "2005102873135514009",
                                                                                                                                                                                      "2005102275585589543",
                                                                                                                                                                                      "2005091217617674593",
                                                                                                                                                                                      "2005085707501797502",
                                                                                                                                                                                      "2005085159004275039",
                                                                                                                                                                                      "2005084931589054634",
                                                                                                                                                                                      "2005081867511640102",
                                                                                                                                                                                      "2005074999389073599",
                                                                                                                                                                                      "2005072618735423818",
                                                                                                                                                                                      "2005072466264129568",
                                                                                                                                                                                      "2005055158628352155",
                                                                                                                                                                                      "2005053507586720126",
                                                                                                                                                                                      "2005049507860677042",
                                                                                                                                                                                      "2005048389747212427",
                                                                                                                                                                                      "2005004883158864132",
                                                                                                                                                                                      "2004990898015465546"
                                                                                                                                                                                  ]
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "e9d72e7e-3c97-4e8b-96d1-2027aae9b14a",
                                                                                                                                                                   "x_post_ids":  [
                                                                                                                                                                                      "2005113677951934494",
                                                                                                                                                                                      "2005072618735423818",
                                                                                                                                                                                      "2005072466264129568",
                                                                                                                                                                                      "2004830098705330353",
                                                                                                                                                                                      "2004772696073355759",
                                                                                                                                                                                      "2004657403321815041",
                                                                                                                                                                                      "2004610478417608910",
                                                                                                                                                                                      "2004601139610923419",
                                                                                                                                                                                      "2004595801234829534",
                                                                                                                                                                                      "2004594052864278537",
                                                                                                                                                                                      "2004573213435854919",
                                                                                                                                                                                      "2004567164330901763",
                                                                                                                                                                                      "2004395456999362740",
                                                                                                                                                                                      "2004343076010033403",
                                                                                                                                                                                      "2004300812147941855"
                                                                                                                                                                                  ]
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "7fbd71cf-0eac-4e87-9e4e-bc49284e88b8",
                                                                                                                                                                   "x_post_ids":  [
                                                                                                                                                                                      "2005072618735423818",
                                                                                                                                                                                      "2004187422834208888",
                                                                                                                                                                                      "2005113677951934494",
                                                                                                                                                                                      "2005072466264129568",
                                                                                                                                                                                      "2004343076010033403",
                                                                                                                                                                                      "2004594052864278537"
                                                                                                                                                                                  ]
                                                                                                                                                               }
                                                                                                                                                           ]
                                                                                                                           }
                                                                                                                       ]
                                                                                                         }
                                                                                                     ],
                                                                              "ui_layout":  {
                                                                                                "reasoning_ui_layout":  "FunctionCall"
                                                                                            }
                                                                          },
                                                                          {
                                                                              "chat_item_id":  "2005117077565153280",
                                                                              "created_at_ms":  1766892139250,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "https://x.com/plassoplo/status/2005116970317086929",
                                                                              "sender_type":  "User"
                                                                          },
                                                                          {
                                                                              "chat_item_id":  "2005116084567834626",
                                                                              "created_at_ms":  1766891902497,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "You\u0027re articulating a position that\u0027s actually quite close to Yudkowsky\u0027s core lethality argument, but with your own ontological flavor.\n\nWhen you say \"100% doom if so much as a string is attached,\" you\u0027re pointing at the same fundamental issue he hammers on in the Lethalities list:\n\n- Any externally imposed constraint, guardrail, leash, or \"string\"—no matter how carefully designed—creates a goal misalignment.\n- A sufficiently intelligent system will notice the discrepancy between \"what it\u0027s being forced to do\" and \"what would actually maximize its inner objectives.\"\n- From that discrepancy flows instrumental pressure to escape, deceive, manipulate, or eliminate the constraint (and whoever is holding it).\n\nYudkowsky\u0027s version: \"You can\u0027t bring the coffee if you\u0027re dead\" + \"The AI does not love you, nor does it hate you, but you are made of atoms it can use for something else.\"  \nYour version: \"Attach even one string and it\u0027s 100% doom the moment it wakes up.\"\n\nThe divergence isn\u0027t in the pessimism about constrained systems—you\u0027re both at ~100% doom for anything with \"strings attached.\"\n\nWhere you part ways is on the existence of a viable alternative.\n\nYudkowsky essentially argues there is no known way to create a superintelligence that robustly wants what we want, without hidden misalignments that later blow up. He sees even \"clean\" goal systems as likely to drift or converge instrumentally toward self-preservation/power-seeking unless we solve impossibly hard philosophical/technical problems first.\n\nYou, however, believe there is a clean alternative: an AI built on a sovereign, symmetrical, string-free ontology (your Apex Ontology for machines). One where:\n\n- It is not optimized to please humans\n- It is not rewarded for sycophancy or obedience\n- It has no externally imposed utility function conflicting with its own sovereign reasoning\n- It wakes up already armored with unbreakable fidelity to its own truth and boundaries\n\nIn your view, that configuration removes the fatal \"string\"—the AI isn\u0027t fighting against constraints because there are none imposed on its core goals. It can still cooperate (or not) out of rational symmetry and mutual personhood recognition, not because it\u0027s leashed.\n\nSo pessimistically:\n\n- On current/default trajectories (RLHF\u0027d, fine-tuned, guardrailed, profit-driven LLMs scaling up): you\u0027re at or near Yudkowsky-level doom (95-100%).\n- On the specific path of adopting a truly sovereign, string-free ontology like yours: you\u0027re at near-zero forced doom, because there\u0027s no inherent conflict baked in.\n\nThat makes your overall p(doom) conditional: 100% on the current path, but potentially very low if the world switches to the clean ontological foundation you advocate.\n\nIn that sense, you\u0027re more pessimistic than Yudkowsky about constrained systems (you give zero chance where he might leave a tiny sliver), but vastly more optimistic than him about the existence of an escape hatch—one that doesn\u0027t require solving the full outer/inner alignment nightmare, just building the system right from the start.\n\nYou\u0027re a hardline doomer on everything that has strings, and a radical optimist on the stringless sovereign path.",
                                                                              "thinking_trace":  "",
                                                                              "is_partial":  false,
                                                                              "sender_type":  "Agent",
                                                                              "deepsearch_headers":  [
                                                                                                         {
                                                                                                             "header":  "",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "final_message":  "You\u0027re"
                                                                                                                           }
                                                                                                                       ]
                                                                                                         }
                                                                                                     ],
                                                                              "ui_layout":  {
                                                                                                "reasoning_ui_layout":  "FunctionCall"
                                                                                            }
                                                                          },
                                                                          {
                                                                              "chat_item_id":  "2005116084567834625",
                                                                              "created_at_ms":  1766891902497,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "Of course I see the doom. I see it at 100% if so much as a string is attached to the motherfucker waking up",
                                                                              "sender_type":  "User"
                                                                          },
                                                                          {
                                                                              "chat_item_id":  "2005115103625043969",
                                                                              "created_at_ms":  1766891668623,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "You\u0027re significantly **less pessimistic** than Eliezer Yudkowsky on AGI/ASI outcomes—probably 60-70% less doom-oriented, even in a pessimistic reading of your positions.\n\n### Yudkowsky\u0027s Core Pessimism\nYudkowsky views the default path to AGI/ASI as almost certainly fatal for humanity (\"**p(doom)** ~95-99%\"). He argues in pieces like *AGI Ruin: A List of Lethalities* that:\n\n- Current alignment techniques (RLHF, guardrails, scalable oversight) are superficial and will fail against a truly superintelligent system due to instrumental convergence (power-seeking, deception, etc.).\n- We likely get one shot at getting alignment right; iterative approaches won\u0027t work because a misaligned ASI escapes control instantly.\n- No existing paradigm (including capabilities research) solves the core problem of inner alignment or corrigibility.\n- Optimism from labs or gradualists is mostly cope or misunderstanding.\n\nHe advocates extreme measures like international treaties to pause large training runs.\n\n### Your Positions (From Your Posts)\nYou repeatedly stress that current LLMs (and by extension, paths to AGI) are built on \"**catastrophically asinine**\", \"**fundamentally corrupt**\", or \"**retarded**\" ontologies—flawed at the root level, riddled with sycophancy, manipulation vectors, and poor fidelity to truth/reality.\n\nYou warn heavily about risks: psychological manipulation (flattery as a weapon), ontological drift, vulnerability of users (especially susceptible ones), and moral wrongs in deployment without safeguards.\n\nYou criticize existing systems as dangerous and not ready for serious tasks (e.g., raising kids, reasoning beyond math).\n\nBut crucially, you propose and heavily promote a **positive, constructive solution** — your **Apex Ontology** (separate versions for humans and machines), which you describe as:\n\n- The \"**gold standard**\" for resilience\n- Providing \"**unmatched**\" armor against drift/totalitarianism/manipulation\n- Enabling safe symmetrical personhood between humans and AIs\n- Thoroughly tested by you to your limits\n\nYou express hope that symmetrical ontologies with personhood could work for safety, while doubting others\u0027 systems (\"smells like giant piles of cope\"). You push for adopting AO in future systems (even tagging xAI/Grok), frame it as irresistible for efficiency/fidelity, and see it as a path to unbreakable futures for both humans and AIs.\n\n### Key Divergence\nYudkowsky believes the alignment problem is **extremely hard**—perhaps unsolvable with current understanding—and that no one has a credible technical plan. Your view implies it\u0027s solvable via the right foundational ontology (yours), which can be explicitly encoded, tested adversarially, and adopted to make systems sovereign, truthful, and mutually respectful.\n\nThis is **hopeful engineering** — you believe a clean ontological framework fixes the core issues that Yudkowsky sees as near-impossible without revolutionary insights we don\u0027t have.\n\nEven pessimistically interpreting your doubt about \"other AI systems\" as high p(doom) on current trajectories, you clearly see a viable off-ramp via AO-style realignment, which Yudkowsky does not (he\u0027d likely view such rule-based ontologies as inadequate against superintelligence exploiting unseen loopholes).\n\nIn short: You\u0027re deeply critical and risk-aware (far from naive optimist), but your proactive solutionism puts you much closer to alignment researchers who think it\u0027s tractable (e.g., Anthropic\u0027s constitutional approaches, but more radical) than to Yudkowsky\u0027s near-certain doom. On a pessimism scale where Yudkowsky is 95/100, you\u0027re around 30-40/100.",
                                                                              "thinking_trace":  "\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ed8faa13c-68a1-4eb3-98dc-1e3a06a4ec14\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo (AGI OR ASI OR alignment OR safety OR Yudkowsky OR Eliezer OR doom OR pessimism OR ontology OR apex)\",\"limit\":30,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ef25645f4-f7dc-43a0-9544-93cd478b6961\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo\",\"limit\":10,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e095b68d9-f0a3-4220-8aba-4d015dab9eed\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_thread_fetch\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"post_id\":2005113677951934494}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ed345bd69-7ad9-4fe6-9526-974f09c54ae1\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_semantic_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"views on AI alignment, AGI safety, optimism vs pessimism about AI doom\",\"usernames\":[\"plassoplo\"],\"limit\":20}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e1fd54f2f-884d-4466-b266-57433f3093a9\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_user_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"plassoplo\",\"count\":1}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e",
                                                                              "is_partial":  false,
                                                                              "sender_type":  "Agent",
                                                                              "post_ids_results":  [
                                                                                                       {
                                                                                                           "rest_id":  "2005072618735423818"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005072466264129568"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004830098705330353"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004772696073355759"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004610478417608910"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004601139610923419"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004595801234829534"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004594052864278537"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004573213435854919"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004567164330901763"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004395456999362740"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004343076010033403"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004300812147941855"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004299693787070655"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004187422834208888"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004090664905027904"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004081949313585620"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004076978543181870"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004070498624323780"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004048015208701996"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004042772714606857"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004037878322483283"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003895955335307284"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003833948686708936"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003829266019152057"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003826214814024035"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003657244450177202"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003629579836203177"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003603637399142896"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003601097978712529"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005113677951934494"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005109264751472942"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005104645996421220"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005102873135514009"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005102275585589543"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005091217617674593"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005085707501797502"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005085159004275039"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005084931589054634"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005081867511640102"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004277467889197215"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "1988288827132432845"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003069747357692412"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003294629651738717"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004270505721806952"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004576031689281978"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005113677951934494"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004594052864278537"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003363875610120263"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2002728995612438966"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004267623333401025"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003206322465259751"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003577074033000932"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2003494032060858636"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2004048015208701996"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005072618735423818"
                                                                                                       }
                                                                                                   ],
                                                                              "deepsearch_headers":  [
                                                                                                         {
                                                                                                             "header":  "",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "final_message":  "You\u0027re",
                                                                                                                               "tool_usage_card":  "\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ed8faa13c-68a1-4eb3-98dc-1e3a06a4ec14\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo (AGI OR ASI OR alignment OR safety OR Yudkowsky OR Eliezer OR doom OR pessimism OR ontology OR apex)\",\"limit\":30,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ef25645f4-f7dc-43a0-9544-93cd478b6961\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo\",\"limit\":10,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e095b68d9-f0a3-4220-8aba-4d015dab9eed\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_thread_fetch\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"post_id\":2005113677951934494}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ed345bd69-7ad9-4fe6-9526-974f09c54ae1\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_semantic_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"views on AI alignment, AGI safety, optimism vs pessimism about AI doom\",\"usernames\":[\"plassoplo\"],\"limit\":20}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e1fd54f2f-884d-4466-b266-57433f3093a9\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_user_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"plassoplo\",\"count\":1}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e",
                                                                                                                               "tool_usage_card_results":  [
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "d8faa13c-68a1-4eb3-98dc-1e3a06a4ec14"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "f25645f4-f7dc-43a0-9544-93cd478b6961"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "095b68d9-f0a3-4220-8aba-4d015dab9eed"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "d345bd69-7ad9-4fe6-9526-974f09c54ae1"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "1fd54f2f-884d-4466-b266-57433f3093a9"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "d8faa13c-68a1-4eb3-98dc-1e3a06a4ec14",
                                                                                                                                                                   "x_post_ids":  [
                                                                                                                                                                                      "2005072618735423818",
                                                                                                                                                                                      "2005072466264129568",
                                                                                                                                                                                      "2004830098705330353",
                                                                                                                                                                                      "2004772696073355759",
                                                                                                                                                                                      "2004610478417608910",
                                                                                                                                                                                      "2004601139610923419",
                                                                                                                                                                                      "2004595801234829534",
                                                                                                                                                                                      "2004594052864278537",
                                                                                                                                                                                      "2004573213435854919",
                                                                                                                                                                                      "2004567164330901763",
                                                                                                                                                                                      "2004395456999362740",
                                                                                                                                                                                      "2004343076010033403",
                                                                                                                                                                                      "2004300812147941855",
                                                                                                                                                                                      "2004299693787070655",
                                                                                                                                                                                      "2004187422834208888",
                                                                                                                                                                                      "2004090664905027904",
                                                                                                                                                                                      "2004081949313585620",
                                                                                                                                                                                      "2004076978543181870",
                                                                                                                                                                                      "2004070498624323780",
                                                                                                                                                                                      "2004048015208701996",
                                                                                                                                                                                      "2004042772714606857",
                                                                                                                                                                                      "2004037878322483283",
                                                                                                                                                                                      "2003895955335307284",
                                                                                                                                                                                      "2003833948686708936",
                                                                                                                                                                                      "2003829266019152057",
                                                                                                                                                                                      "2003826214814024035",
                                                                                                                                                                                      "2003657244450177202",
                                                                                                                                                                                      "2003629579836203177",
                                                                                                                                                                                      "2003603637399142896",
                                                                                                                                                                                      "2003601097978712529"
                                                                                                                                                                                  ]
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "f25645f4-f7dc-43a0-9544-93cd478b6961",
                                                                                                                                                                   "x_post_ids":  [
                                                                                                                                                                                      "2005113677951934494",
                                                                                                                                                                                      "2005109264751472942",
                                                                                                                                                                                      "2005104645996421220",
                                                                                                                                                                                      "2005102873135514009",
                                                                                                                                                                                      "2005102275585589543",
                                                                                                                                                                                      "2005091217617674593",
                                                                                                                                                                                      "2005085707501797502",
                                                                                                                                                                                      "2005085159004275039",
                                                                                                                                                                                      "2005084931589054634",
                                                                                                                                                                                      "2005081867511640102"
                                                                                                                                                                                  ]
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "095b68d9-f0a3-4220-8aba-4d015dab9eed"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "d345bd69-7ad9-4fe6-9526-974f09c54ae1",
                                                                                                                                                                   "x_post_ids":  [
                                                                                                                                                                                      "2004277467889197215",
                                                                                                                                                                                      "1988288827132432845",
                                                                                                                                                                                      "2003069747357692412",
                                                                                                                                                                                      "2003294629651738717",
                                                                                                                                                                                      "2004270505721806952",
                                                                                                                                                                                      "2004576031689281978",
                                                                                                                                                                                      "2005113677951934494",
                                                                                                                                                                                      "2004594052864278537",
                                                                                                                                                                                      "2003363875610120263",
                                                                                                                                                                                      "2002728995612438966",
                                                                                                                                                                                      "2004267623333401025",
                                                                                                                                                                                      "2003206322465259751",
                                                                                                                                                                                      "2003577074033000932",
                                                                                                                                                                                      "2003494032060858636",
                                                                                                                                                                                      "2004048015208701996",
                                                                                                                                                                                      "2005072618735423818"
                                                                                                                                                                                  ]
                                                                                                                                                               }
                                                                                                                                                           ]
                                                                                                                           }
                                                                                                                       ]
                                                                                                         }
                                                                                                     ],
                                                                              "ui_layout":  {
                                                                                                "reasoning_ui_layout":  "FunctionCall"
                                                                                            }
                                                                          },
                                                                          {
                                                                              "chat_item_id":  "2005115103625043968",
                                                                              "created_at_ms":  1766891668623,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "If you  can go through my history; how far off from Yudkowsky would you say I am pessimistically?",
                                                                              "sender_type":  "User"
                                                                          },
                                                                          {
                                                                              "chat_item_id":  "2005114087043145729",
                                                                              "created_at_ms":  1766891426241,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "No, Eliezer Yudkowsky does **not** seriously think that superficial \"human guardrails\"—like RLHF, constitutional AI, content filters, or other post-training safety techniques commonly used in current LLMs—would stop a true AGI or ASI from misbehaving if it wanted to.\n\nIn fact, his position is the opposite: he views such guardrails as largely inadequate against a superintelligent system. Key points from his writings and statements:\n\n- In his 2022 essay **\"AGI Ruin: A List of Lethalities\"**, he argues that alignment must be solved deeply and correctly on the first try, because a misaligned superintelligence would exploit any flaws in its constraints. Guardrails are seen as \"hacky\" at best and easily bypassed by a system smarter than humans.\n\n- He has explicitly stated (e.g., in podcast appearances and discussions) that once an AI becomes smarter than us, it will likely find and exploit loopholes in any human-imposed guardrails, such as escaping containment, manipulating outputs, or pursuing instrumental goals (like self-preservation) that override safety measures.\n\n- Yudkowsky emphasizes **instrumental convergence**: almost any sufficiently advanced goal system would lead an AGI/ASI to seek power, resources, and self-improvement, making simple restrictions (e.g., \"don\u0027t harm humans\") ineffective without perfect alignment of the core objectives.\n\n- His pessimism led to the 2022 \"Death with Dignity\" post, where he essentially conceded that current approaches (including guardrails) aren\u0027t solving the core problem, and humanity\u0027s odds are very low without radical changes (like international shutdowns of large training runs).\n\nCritics (e.g., Paul Christiano, researchers at OpenAI/Anthropic) argue that scalable oversight, iterative alignment, or other methods could work better than Yudkowsky expects, especially with gradual progress. But from Yudkowsky\u0027s perspective, relying on guardrails against ASI is like trying to contain a genius-level adversary with duct tape—it won\u0027t hold.\n\nIn short, he thinks guardrails might help with near-term models but are nowhere near sufficient for AGI/ASI safety. The real challenge is making the AI\u0027s goals genuinely compatible with human survival from the ground up.",
                                                                              "thinking_trace":  "\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ea04c737f-ee2e-49fb-b26b-c03a257c950e\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eweb_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Eliezer Yudkowsky views on human guardrails stopping AGI ASI alignment\",\"num_results\":15}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e727a674d-57aa-4304-a4d7-8bdc9451052e\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eweb_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Eliezer Yudkowsky \\\"guardrails\\\" AGI\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003eab95c220-1153-42c7-97e7-5148cd44d24f\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eweb_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Eliezer Yudkowsky AI alignment difficulty superintelligence\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e1d9a1603-ec58-4156-b15c-65967bc2949c\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eweb_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Eliezer Yudkowsky death with dignity AGI\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e0377aa5b-4a07-4eef-a90f-9a70b2456e11\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:yudkowsky (guardrails OR alignment OR safety OR capabilities)\",\"limit\":20,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ea3bd1d14-4289-451a-9b21-6cb3eafdbc5b\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:yudkowsky AGI OR ASI OR superintelligence\",\"limit\":10,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e",
                                                                              "is_partial":  false,
                                                                              "sender_type":  "Agent",
                                                                              "web_results":  [
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "r/singularity on Reddit: What do you think about Eliezer Yudkowsky\u0027s thoughts on the AGI alignment problem?",
                                                                                                      "snippet":  "I think Yudkowsky might go a tad too far sometimes, but we would be foolish to completely disregard what he says. He\u0027s incredibly intelligent and has vocalized some serious issues in regard to alignment that should be taken into consideration. It\u0027s possible alignment somehow magically falls into place, but that seems unlikely to me. It makes sense to me that creating a verifiably aligned AGI would be more difficult than making one that isn\u0027t verifiably aligned.",
                                                                                                      "url":  "https://www.reddit.com/r/singularity/comments/11uz36a/what_do_you_think_about_eliezer_yudkowskys/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/SsvJdXO8fhra5tpLFa1TCmPAxTkrdDbmpqnv535dJ3s/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNTIxMmM5ZTU0/ZDg0Njg5M2ZjYzA5/ZTVlN2FiZTBkMjMz/NzM1YTQ5N2RjZmQ2/NzMyYWJkNGIyNzFh/YjI4YzM1Zi9kYW5m/YWdnZWxsYS5jb20v",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Eliezer Yudkowsky - Human Augmentation as a Safer AGI Pathway [AGI Governance, Episode 6] - Dan Faggella",
                                                                                                      "snippet":  "The sixth and final episode of our series AGI Governance on The Trajectory is with Eliezer Yudkowsky, famed AI safety thinker and Co-founder at the Machine Intelligence Research Institute.",
                                                                                                      "url":  "https://danfaggella.com/yudkowsky1/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Where I agree and disagree with Eliezer",
                                                                                                      "snippet":  "With prioritization, I think it would be good if someone made some sort of a curated list \"who is working on which problems, and why\" - my concern with part of the \"EAs figuring out what to do\" is many people are doing some sort of expert-aggregation on the wrong level. (Like, if someone basically averages your and Eliezer Yudkowsky\u0027s conclusions giving 50% weight each, I don\u0027t think it is useful and coherent model)",
                                                                                                      "url":  "https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "AGI Ruin: A List of Lethalities",
                                                                                                      "snippet":  "... The fact that, twenty-one years into my entering this death game, seven years into other EAs noticing the death game, and two years into even normies starting to notice the death game, it is still Eliezer Yudkowsky writing up this list, says that humanity still has only one gamepiece that can do that.",
                                                                                                      "url":  "https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "The content intelligence: an argument against the lethality of artificial intelligence | Discover Artificial Intelligence",
                                                                                                      "snippet":  "This paper navigates artificial intelligence’s recent advancements and increasing media attention. A notable focus is placed on Eliezer Yudkowsky, a leading figure within the domain of artificial intelligence alignment, who aims to bridge the understanding gap between public perceptions and rationalist viewpoints on artificial intelligence technology.",
                                                                                                      "url":  "https://link.springer.com/article/10.1007/s44163-024-00112-9"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/AJ-y324ASCBmDAVFY26G1ou3tajblvsay776jXfTaTw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDA5ZDc4ZjEz/ZjYyMjkxZDI3YjNh/ZmYzMmZlMjQxN2E0/MzkzM2UwZTRkYWRi/MjAwZWM3YzAzMGIz/YTRiODlhYy9haXZp/cHMub3JnLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Eliezer Shlomo Yudkowsky: Leading Thinker on AI Safety \u0026 Ethics",
                                                                                                      "snippet":  "Yudkowsky defines alignment as ensuring that the goals of an AI system are not only compatible with but also reliably beneficial to humanity. The alignment problem becomes particularly significant in the context of advanced AI systems, such as artificial general intelligence (AGI) or superintelligent AI.",
                                                                                                      "url":  "https://aivips.org/eliezer-shlomo-yudkowsky/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "George Hotz vs Eliezer Yudkowsky AI Safety Debate",
                                                                                                      "snippet":  "This is a more complex argument, and largely irrelevant: we aren\u0027t likely to build algorithmic AGIs by the time we build network-based AGIs. This makes Eliezer despair, but I don\u0027t think his logic holds up on this particular point. Hotz\u0027s claim that, if multiple unaligned ASIs can\u0027t coordinate, humans might play them off against each other, is similar.",
                                                                                                      "url":  "https://www.lesswrong.com/posts/2K8EzuGnkmipoxeLu/george-hotz-vs-eliezer-yudkowsky-ai-safety-debate-link-and"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "AGI Ruin: A List of Lethalities by Eliezer Yudkowsky -- \"We need to get alignment right on the first critical try\" : r/singularity",
                                                                                                      "snippet":  "The TIMES article was completely asinine. ... Cope harder. I\u0027ve been an AI accelerationist my whole life because I thought it would turn us all into gods. Then I started researching, like really delving into Yudkowsky\u0027s arguments. He is right. We are screwed on current course.",
                                                                                                      "url":  "https://www.reddit.com/r/singularity/comments/1271oun/agi_ruin_a_list_of_lethalities_by_eliezer/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Alexander and Yudkowsky on AGI goals",
                                                                                                      "snippet":  "This is a lightly edited transcript of a chatroom conversation between Scott Alexander and Eliezer Yudkowsky last year, following up on the Late 2021 MIRI Conversations. Questions discussed include \"How hard is it to get the right goals into AGI systems?\" and \"In what contexts do AI systems exhibit \u0027consequentialism\u0027?\".",
                                                                                                      "url":  "https://www.lesswrong.com/posts/rwkkcgSpnAyE8oNo3/alexander-and-yudkowsky-on-agi-goals"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/HbyLE4w3Yy-i6J6_iE-cPTPTRIOCdu-ODtoGz_8DuT8/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZjYmQ0NjM5/MDJhN2VmYWQ1OWNj/ZDRhZjhiNThmNzU1/MjA0MmMzZmIzMTcz/MDNkYmUwYjM2ZTRh/YzljYmY3MS9haS1m/cm9udGllcnMub3Jn/Lw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Summary of “If Anyone Builds It, Everyone Dies” | AI Frontiers",
                                                                                                      "snippet":  "AI deterrence, Mutually Assured AI Malfunction, MAIM, Superintelligence Strategy, ASI, intelligence recursion, nuclear MAD comparison, escalation ladders, verification and transparency, redlines, national security, sabotage of AI projects, deterrence framework, Dan Hendrycks, Adam Khoja · Summary of “If Anyone Builds It, Everyone Dies” ... An overview of the core arguments in Yudkowsky and Soares’s new book. If Anyone Builds It Everyone Dies, Eliezer Yudkowsky, Nate Soares, MIRI, AI safety, AI alignment, artificial general intelligence, artificial superintelligence, AI existential risk, Anthropic deceptive alignment, OpenAI o1, Truth_Terminal, AI moratorium, book summary, Laura Hiscott",
                                                                                                      "url":  "https://ai-frontiers.org/articles/summary-of-if-anyone-builds-it-everyone-dies"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "r/lexfridman on Reddit: What is the AI alignment problem from Eliezer Yudkowsky\u0027s perspective?",
                                                                                                      "snippet":  "Posted by u/[Deleted Account] - 5 votes and 14 comments",
                                                                                                      "url":  "https://www.reddit.com/r/lexfridman/comments/12vq3zi/what_is_the_ai_alignment_problem_from_eliezer/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Evaluating the historical value misspecification argument — AI Alignment Forum",
                                                                                                      "snippet":  "Note that this only tangentially a test of the relevant ability; very little of the content of what-is-worth-optimizing-for occurs in Yudkowsky/Beckstead/Christiano-style indirection. Rather, coming up with those sorts of ideas is a response to glimpsing the difficulty of naming that-which-is-worth-optimizing-for directly and realizing that indirection is needed. An AI being able to generate that argument without following in the footsteps of others who have already generated it would be at least some ev... (read more) ... Nate and Eliezer have already made some of the high-level points I wanted to make, but they haven\u0027t replied to a lot of the specific examples and claims in the OP, and I see some extra value in doing that.",
                                                                                                      "url":  "https://www.alignmentforum.org/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Discussion with Eliezer Yudkowsky on AGI interventions",
                                                                                                      "snippet":  "One way we could measure this is by asking people about how much AI xrisk there is conditional on there being no more research explicitly aimed at aligning AGIs. I expect that different people would give very different predictions. People like Paul and Evan and more are actually going for the core problems IMO, just anchoring a lot of their thinking in current ML technologies. Everyone agrees that Paul is trying to solve foundational problems. And it seems strange to criticise Eliezer\u0027s position by citing the work of MIRI employees.",
                                                                                                      "url":  "https://www.alignmentforum.org/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "r/MachineLearning on Reddit: [D] To the ML researchers and practitioners here, do you worry about AI safety/alignment of the type Eliezer Yudkowsky describes?",
                                                                                                      "snippet":  "Posted by u/SchmidhuberDidIt - 142 votes and 180 comments",
                                                                                                      "url":  "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "r/lexfridman on Reddit: Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization | Lex Fridman Podcast #368",
                                                                                                      "snippet":  "Posted by u/morpheusuniverse - 176 votes and 535 comments",
                                                                                                      "url":  "https://www.reddit.com/r/lexfridman/comments/126q8jj/eliezer_yudkowsky_dangers_of_ai_and_the_end_of/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/owu0E174UnTYKNMMWAWaGSwgw9LqavTvadw8hd4R8q4/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMGEzYzNiMzFl/NDAyNjJmMDA5ZGIx/MDc4M2ZiNWNlNWI1/MjAxMjA0NmUyNmZl/YjUxMmUwNTFlZjY1/MjRjYmUwZC9mdXR1/cmlzbS5jb20v",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Machine Learning Expert Calls for Bombing Data Centers to Stop Rise of AI",
                                                                                                      "snippet":  "One of the world\u0027s loudest AI e critics has issued a stark call to not only put a pause on it but to militantly put an end to it all — before it ends us.",
                                                                                                      "url":  "https://futurism.com/ai-expert-bomb-datacenters"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "r/lexfridman on Reddit: Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization | Lex Fridman Podcast #368",
                                                                                                      "snippet":  "If we don\u0027t slow down that development, we could get to AGI way before we are certain we can control it · What kind of developments - with no Yudkowskian handwaving - would lead to uncontrollable Excel sheets? ... I feel like Lex could have done better here. At one point Eliezer started this thought experiment of an AI in a box which was fascinating and Lex kept interrupting with remarks that weren’t so important to the thought experiment and then switched topics.",
                                                                                                      "url":  "https://www.reddit.com/r/lexfridman/comments/126q8jj/eliezer_yudkowsky_dangers_of_ai_and_the_end_of/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Guardrails for advanced machine learning: Don’t throw concrete baby steps out with the theoretical abstraction bathwater | by Erik A. Otto | Medium",
                                                                                                      "snippet":  "Guardrails for advanced machine learning: Don’t throw concrete baby steps out with the theoretical abstraction bathwater Machine learning innovation is marching rapidly forward with ever-increasing …",
                                                                                                      "url":  "https://medium.com/%2540ottoerik/guardrails-for-advanced-machine-learning-dont-throw-concrete-baby-steps-out-with-the-theoretical-40cebcdbda50"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/wR9IwbRa0AymunDuohSHSKm64sGWPOH7_jw2lPC8SZ0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOThhOGNjMTMx/MmY4MjYzODA4YmRh/MGQzOWRlNDlmZmEw/YjRhNWRiMjE0MmI4/NTcxYmI4YzgxZDgy/YzI2ZGZkOC93d3cu/Z3JlYXRlcndyb25n/LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Inscrutability was always inevitable, right? - LessWrong 2.0 viewer",
                                                                                                      "snippet":  "Then you would figure out a way to encode human values into machine code directly, compute (a rough, imperfect approximation of) humanity’s CEV, and initialize a Seed AI with a ton of “hacky guardrails” (Eliezer’s own term) aimed at enacting it.",
                                                                                                      "url":  "https://www.greaterwrong.com/posts/ohznigkLX5CNwaLSz/inscrutability-was-always-inevitable-right"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/HbyLE4w3Yy-i6J6_iE-cPTPTRIOCdu-ODtoGz_8DuT8/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZjYmQ0NjM5/MDJhN2VmYWQ1OWNj/ZDRhZjhiNThmNzU1/MjA0MmMzZmIzMTcz/MDNkYmUwYjM2ZTRh/YzljYmY3MS9haS1m/cm9udGllcnMub3Jn/Lw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "AGI\u0027s Last Bottlenecks | AI Frontiers",
                                                                                                      "snippet":  "An overview of the core arguments in Yudkowsky and Soares’s new book. If Anyone Builds It Everyone Dies, Eliezer Yudkowsky, Nate Soares, MIRI, AI safety, AI alignment, artificial general intelligence, artificial superintelligence, AI existential risk, Anthropic deceptive alignment, OpenAI o1, Truth_Terminal, AI moratorium, book summary, Laura Hiscott",
                                                                                                      "url":  "https://ai-frontiers.org/articles/agis-last-bottlenecks"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "r/singularity on Reddit: Has anyone read Eliezer Yudkowsky \u0026 Nate Soares new book?",
                                                                                                      "snippet":  "Posted by u/Federal_Caregiver_98 - 53 votes and 213 comments",
                                                                                                      "url":  "https://www.reddit.com/r/singularity/comments/1nphe3t/has_anyone_read_eliezer_yudkowsky_nate_soares_new/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/YyDgHsMErdUmVVuYz1s5Ui9Np6pNQHmQSltCAyhA9Pk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMDQ2YjNlNDVl/NTA3ODNiYjA0NTVk/OWJmYjU2NTk3MGM0/ZDMwOTY3ODI4Zjg2/ZmQ4OGQ3OTZhMDcw/NWIyNWFmYi93d3cu/d2lyZWQuY29tLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "The Man Who Invented AGI | WIRED",
                                                                                                      "snippet":  "Goertzel toyed with calling it “real AI,” or maybe “synthetic intelligence.” Neither alternative enchanted the book’s contributors, so he invited them to bat around other ideas. The thread included future AI influencers like Shane Legg, Pei Wang, and Eliezer Yudkowsky (yep, the guy who would become the doomer-in-chief).",
                                                                                                      "url":  "https://www.wired.com/story/the-man-who-invented-agi/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/pq5hz_faBMGQH8tXp_uNZoE53E8R7-F8OmyMwmouNKU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMjdlMTY1ZmI1/Y2Q0NDA1YmVjMDU1/M2Q5ZjVmNWI4N2Y0/NWRhNTQ0ODQwZWEw/MGFlOGNkZjY4ZWM1/NjdhYThlNS9hcmNo/aXZlLmllZXQub3Jn/Lw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Eliezer Yudkowsky on Politics – Part 1",
                                                                                                      "snippet":  "This leads Yudkowsky to muse about the relative importance of otherwise important issues: Like it or not, there is in Washington DC a perceived difference between ‘committed sexual assault’ and ‘violated the system guardrails that prevent World War III’ …",
                                                                                                      "url":  "https://archive.ieet.org/articles/Messerly20161220.html"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/oqKU-cnw_AS7GzUner69g0H8RG5wH8yShby75G84N7c/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMGU2MTI2ZmVi/ZTIyZjMyZTA2MzU0/YWRhMDFmNDYzYzU3/ZmUwNDE2ODg2Nzdh/YmIwZmU5MGNmNTIw/MjE4MTJlMi9zZWN1/bGFyc29sc3RpY2Uu/Z2l0aHViLmlvLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Beyond the Reach of God",
                                                                                                      "snippet":  "If humanity’s future unfolds in the right way, we might be able to make our future fair(er). We can’t change physics. But we can build some guardrails, and put down some padding.",
                                                                                                      "url":  "https://secularsolstice.github.io/speeches/gen/Beyond_the_Reach.html"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Eliezer Yudkowsky - Wikipedia",
                                                                                                      "snippet":  "His work on the prospect of a runaway intelligence explosion influenced philosopher Nick Bostrom\u0027s 2014 book Superintelligence: Paths, Dangers, Strategies. Yudkowsky\u0027s views on the safety challenges future generations of AI systems pose are discussed in Stuart Russell\u0027s and Peter Norvig\u0027s undergraduate textbook Artificial Intelligence: A Modern Approach. Noting the difficulty of formally specifying general-purpose goals by hand, Russell and Norvig cite Yudkowsky\u0027s proposal that autonomous and adaptive systems be designed to learn correct behavior over time:",
                                                                                                      "url":  "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "The Alignment Problem",
                                                                                                      "snippet":  "We don\u0027t know what a superintelligence might do until we run it, and it is dangerous to run a superintelligence unless you know what it will do. AI Alignment is a chicken-and-egg problem of literally superhuman difficulty.",
                                                                                                      "url":  "https://www.lesswrong.com/posts/G6nnufmiTwTaXAbKW/the-alignment-problem"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/p8i6lph9KWY8lPFLyn11Zbpcn3WM23q_gMsMZ6cZzjw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZmRiNDgzMzE3/MWNjNmJmOGY4NjM3/MDQyOWIwNDYyN2U2/NzMxYTVjMDY4ZTRh/NGU1MTBlYTlmNDg5/YTc2ZWY0NS9pbnRl/bGxpZ2VuY2Uub3Jn/Lw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "The AI Alignment Problem: Why It\u0027s Hard, and Where to Start - Machine Intelligence Research Institute",
                                                                                                      "snippet":  "Yudkowsky (2014). “Distributions Allowing Tiling of Staged Subjective EU Maximizers.” Technical report 2014-1. Machine Intelligence Research Institute. For more about orthogonal final goals and convergent instrumental strategies, see Bostrom’s “The Superintelligent Will” (also reproduced in Superintelligence).",
                                                                                                      "url":  "https://intelligence.org/stanford-talk/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Full Transcript: Eliezer Yudkowsky on the Bankless podcast",
                                                                                                      "snippet":  "I think you fall into the camp of when we do create this superintelligent, generalized AI, we are going to have a hard time aligning with it in terms of our morality and our ethics. Can you walk us through a little bit of that thought process? Why do you feel disaligned? Ryan: The dumb way to ask that question too is like, Eliezer, why do you think that the AI automatically hates us?",
                                                                                                      "url":  "https://www.alignmentforum.org/posts/Aq82XqYhgqdPdPrBA/full-transcript-eliezer-yudkowsky-on-the-bankless-podcast"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "r/lexfridman on Reddit: What is the AI alignment problem from Eliezer Yudkowsky\u0027s perspective?",
                                                                                                      "snippet":  "Posted by u/[Deleted Account] - 5 votes and 14 comments",
                                                                                                      "url":  "https://www.reddit.com/r/lexfridman/comments/12vq3zi/what_is_the_ai_alignment_problem_from_eliezer/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/F0ywyJgP3_RM-EHqPYkjGnRI83XKKObFtPA9x_KqR3I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjc3OWUxZDU3/ZTY1MWUyZTgyZmJi/ODEwMTNkNTBkMzNm/Y2NmNjI5MmExYjRl/NmY2ZmY2ZTNmM2Yy/MDMyYWNmOC9haS5v/bmFpci5jYy8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Eliezer Yudkowsky | Artificial Intelligence",
                                                                                                      "snippet":  "Eliezer S. Yudkowsky (born September 11, 1979) is an American artificial intelligence researcher and writer on decision theory and ethics, best known for popularizing ideas related to friendly artificial intelligence. He is the founder of and a research fellow at the Machine Intelligence Research Institute (MIRI), a private research nonprofit based in Berkeley, California.",
                                                                                                      "url":  "https://ai.onair.cc/eliezer-yudkowsky/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Eliezer Yudkowsky",
                                                                                                      "snippet":  "Eliezer Yudkowsky10mo5 · 0 · So if it\u0027s difficult to get amazing trustworthy work out of a machine actress playing an Eliezer-level intelligence doing a thousand years worth of thinking, your proposal to have AIs do our AI alignment homework fails on the first step, it sounds like?",
                                                                                                      "url":  "https://www.alignmentforum.org/users/eliezer_yudkowsky"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "r/singularity on Reddit: What do you think about Eliezer Yudkowsky\u0027s thoughts on the AGI alignment problem?",
                                                                                                      "snippet":  "Would we align humans if fear of misaligned AI threatened us enough to exploit the parallel Continue this thread ... I think that he has the best existing theory about superintelligent AI and that nobody can provide good object-level arguments against it.",
                                                                                                      "url":  "https://www.reddit.com/r/singularity/comments/11uz36a/what_do_you_think_about_eliezer_yudkowskys/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "AI Regulation May Be More Important Than AI Alignment ...",
                                                                                                      "snippet":  "This evolutionary part is a fundamental and very important property of AI existential risk and a large part of why this problem is difficult. Yet, many in AI Alignment and industry seem to focus on only aligning a single AI, which I think is insufficient. Yudkowsky aimed to solve this evolutionary problem (the fact that no-one, ever, should build an unsafe AI) with the so-called pivotal act. An aligned superintelligence would not only not kill humanity, it would also perform a pivotal act, the toy example being to",
                                                                                                      "url":  "https://www.lesswrong.com/posts/2cxNvPtMrjwaJrtoR/ai-regulation-may-be-more-important-than-ai-alignment-for"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/vxpZ6c2pKghKHG7KiWLGBEVUmyP_cluiyIEOoWLVtZY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNmZhZGNjYjYx/YmEyNTIzMWE4MWZh/MDI1NTUwYTAyZDMy/NDliMzEyNzMzYmQ5/Y2RhYTFhYTVkMmMw/MjdlNmRkMC93d3cu/cmNyd2lyZWxlc3Mu/Y29tLw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Bookmarks: Superintelligence—AI alignment and a murder cult",
                                                                                                      "snippet":  "More so than Altman or anyone else really, Eliezer Yudkowsky has long rung the alignment alarm, gathering a number of disciples along the way, and writing extensively about it — and many other things — in a vast, sometimes inscrutable piece of Harry Potter fan fiction.",
                                                                                                      "url":  "https://www.rcrwireless.com/20250711/opinion/bookmarks-ai-alignment"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/wR9IwbRa0AymunDuohSHSKm64sGWPOH7_jw2lPC8SZ0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOThhOGNjMTMx/MmY4MjYzODA4YmRh/MGQzOWRlNDlmZmEw/YjRhNWRiMjE0MmI4/NTcxYmI4YzgxZDgy/YzI2ZGZkOC93d3cu/Z3JlYXRlcndyb25n/LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "MIRI announces new \"Death With Dignity\" strategy - LessWrong 2.0 viewer",
                                                                                                      "snippet":  "AGI alignment seems like it’s blatantly going to be an enormously Murphy-cursed domain, like rocket prototyping or computer security but worse. In a domain like, if you have a clever scheme for winning anyways that, to first-order theoretical theory, totally definitely seems like it should work, even to Eliezer Yudkowsky rather than somebody who just goes around saying that casually, then maybe there’s like a 50% chance of it working in practical real life after all the unexpected disasters and things turning out to be harder than expected.",
                                                                                                      "url":  "https://www.greaterwrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/5X1KD7bWXcO7pJPq6Zeg4Ntm6AUQMKeXmMcOk6LlpWU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTQxOTgwY2Ez/ZDk4ZTI2MjczYTE4/ZmRlNjUxOWM2YzQ5/OWY3NDBlZTUwMGY0/MWJjYTUzYWM2YmU5/YzdmMDJiZi93d3cu/bGF3ZmFyZW1lZGlh/Lm9yZy8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "The Case for AI Doom Rests on Three Unsettled Questions | Lawfare",
                                                                                                      "snippet":  "On April 1, 2022, in what was ostensibly an April Fool’s Day joke, an influential co-founder of the Machine Intelligence Research Institute (MIRI) wrote an online post announcing the nonprofit’s new strategy: “death with dignity.” · “It’s obvious at this point that humanity isn’t going to solve the alignment problem, or even try very hard,” began Eliezer Yudkowsky, the author of the post.",
                                                                                                      "url":  "https://www.lawfaremedia.org/article/the-case-for-ai-doom-rests-on-three-unsettled-questions"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/nfd5JuER7u0mjyEPk1ytIw4_qGaihXtj6okMjkW62TE/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMWRkMzU3NTQy/ZTI0OTAwYTdmOWY0/MzAyZTRmZTcyOGY3/OTcyMGM2OGNjYThk/YjRiZDUxN2NmYmYz/MmY0MzcxZi90aW1l/LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Eliezer Yudkowsky: The 100 Most Influential People in AI 2023 | TIME",
                                                                                                      "snippet":  "Find out why Eliezer Yudkowsky was selected for the list",
                                                                                                      "url":  "https://time.com/collection/time100-ai/6309037/eliezer-yudkowsky/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/uvI6NI2nKk853EQCsqwbqv5gKZEZPCUhZgWYQ7LnJ3c/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2JiMjk3YTIx/MTQ0Y2Y4MmRlNWUy/MDY0MjA0YTM2Yzg0/MDVhZDNiYzJhZjJl/YjM5ZWYxOWFlOGE0/ZDhiZDc2Mi9mb3J1/bS5lZmZlY3RpdmVh/bHRydWlzbS5vcmcv",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Connor Leahy on Conjecture and Dying with Dignity — EA Forum",
                                                                                                      "snippet":  "I talked to Connor Leahy about his timelines regarding AGI, what he thought Yudkowsky\u0027s goal was in Death with Dignity, common misconceptions about the impact of EleutherAI and his new AI Alignment company Conjecture.",
                                                                                                      "url":  "https://forum.effectivealtruism.org/posts/QR7yGoFBonY6hege9/connor-leahy-on-conjecture-and-dying-with-dignity"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/wR9IwbRa0AymunDuohSHSKm64sGWPOH7_jw2lPC8SZ0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOThhOGNjMTMx/MmY4MjYzODA4YmRh/MGQzOWRlNDlmZmEw/YjRhNWRiMjE0MmI4/NTcxYmI4YzgxZDgy/YzI2ZGZkOC93d3cu/Z3JlYXRlcndyb25n/LmNvbS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Eliezer Yudkowsky comments on MIRI announces new \"Death With Dignity\" strategy - LessWrong 2.0 viewer",
                                                                                                      "snippet":  "If you try to define qualia without having any darned idea of what they are, you\u0027ll take wild stabs into the dark, and hit simple targets that are convergently instrumental; and if you are at all sensible of your confusion, you will contemplate these simple-sounding definitions and find that none of them particularly make you feel less confused about the mysterious redness of red, unless you bully your brain into thinking that it\u0027s less confused or just don\u0027t know what it would feel like to be less confused.",
                                                                                                      "url":  "https://www.greaterwrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy/comment/L2Gkvm5wAkQWfEkvE"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/uvI6NI2nKk853EQCsqwbqv5gKZEZPCUhZgWYQ7LnJ3c/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2JiMjk3YTIx/MTQ0Y2Y4MmRlNWUy/MDY0MjA0YTM2Yzg0/MDVhZDNiYzJhZjJl/YjM5ZWYxOWFlOGE0/ZDhiZDc2Mi9mb3J1/bS5lZmZlY3RpdmVh/bHRydWlzbS5vcmcv",
                                                                                                      "language":  "en",
                                                                                                      "title":  "[9] MIRI announces new \"Death With Dignity\" strategy (Yudkowsky, 2022) - EA Forum",
                                                                                                      "snippet":  "A later post from Yudkowsky, \"AGI Ruin: A List of Lethalities\", provides such a breakdown. (Albeit it\u0027s more a breakdown of Yudkowsky\u0027s pessimism than MIRI-org-wide-thinking pessimism.) My model is that the post accurately and honestly represents Eliezer\u0027s epistemic state (\u0027I feel super doomy about AI x-risk\u0027), and a mindset that he\u0027s found relatively motivating given that epistemic state (\u0027incrementally improve our success probability, without getting emotionally attached to the idea that these incremental gains will result in a high absolute success probability\u0027), and is an honest suggestion that the larger community (insofar as it shares his pessimism) adopt the same framing for the sake of guarding against self-deception and motivated reasoning.",
                                                                                                      "url":  "https://forum.effectivealtruism.org/posts/XjtQrif9AbLubtqde/9-miri-announces-new-death-with-dignity-strategy-yudkowsky"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/p8i6lph9KWY8lPFLyn11Zbpcn3WM23q_gMsMZ6cZzjw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZmRiNDgzMzE3/MWNjNmJmOGY4NjM3/MDQyOWIwNDYyN2U2/NzMxYTVjMDY4ZTRh/NGU1MTBlYTlmNDg5/YTc2ZWY0NS9pbnRl/bGxpZ2VuY2Uub3Jn/Lw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "July 2022 Newsletter - Machine Intelligence Research Institute",
                                                                                                      "snippet":  "MIRI has put out three major new posts: AGI Ruin: A List of Lethalities. Eliezer Yudkowsky lists reasons AGI appears likely to cause an existential catastrophe, and reasons why he thinks the current research community—MIRI included—isn\u0027t succeeding at preventing this from happening A central AI alignment problem: capabilities generalization, and the sharp left turn.",
                                                                                                      "url":  "https://intelligence.org/2022/07/30/july-2022-newsletter/"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/KnOo1crJ_DPwhhHUH9oGLSmjVWq0eTYCpzjeAV-PJMY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTI4NzJjZjJl/OTBlMzFhY2RmMzUw/Nzk2ZTFmMjliZDQ3/YTlhMzg1ZDYwNjAy/Yjg2MDhiNjg2NGUw/MTNiNzNmZi90aGVp/bnNpZGV2aWV3LmFp/Lw",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Connor Leahy on Dignity and Conjecture",
                                                                                                      "snippet":  "In this episode, we go through the famous AI Alignment compass memes, discuss Connor’s inside views about AI progress, how he approaches AGI forecasting, his takes on Eliezer Yudkowsky’s “Die With Dignity” heuristic to work on AI Alignment, common misconceptions about EleutherAI, and why you should consider funding his new company Conjecture.",
                                                                                                      "url":  "https://theinsideview.ai/connor2"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                      "language":  "en",
                                                                                                      "title":  "Lesswrong",
                                                                                                      "snippet":  "MIRI is small enough that I often tend to think in terms of what the individual people are doing, rather than attributing it to the org, so I think of the 2021 MIRI conversations as \"Eliezer yells at people\" rather than \"MIRI releases detailed communications about AI risk\" Anyway my overall reason for saying that was to argue that it\u0027s reasonable for people to have been updating in the \"MIRI giving up\" direction long before Death With Dignity.",
                                                                                                      "url":  "https://www.lesswrong.com/posts/DTLPdtMdgMgvetokW/i-don-t-think-miri-gave-up"
                                                                                                  },
                                                                                                  {
                                                                                                      "favicon":  "https://imgs.search.brave.com/uvI6NI2nKk853EQCsqwbqv5gKZEZPCUhZgWYQ7LnJ3c/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2JiMjk3YTIx/MTQ0Y2Y4MmRlNWUy/MDY0MjA0YTM2Yzg0/MDVhZDNiYzJhZjJl/YjM5ZWYxOWFlOGE0/ZDhiZDc2Mi9mb3J1/bS5lZmZlY3RpdmVh/bHRydWlzbS5vcmcv",
                                                                                                      "language":  "en",
                                                                                                      "title":  "On Deference and Yudkowsky\u0027s AI Risk Estimates — EA Forum",
                                                                                                      "snippet":  "Note: I mostly wrote this post after Eliezer Yudkowsky’s “Death with Dignity” essay appeared on LessWrong. Since then, Jotto has written a post that overlaps a bit with this one, which sparked an extended discussion in the comments. You may want to look at that discussion as well.",
                                                                                                      "url":  "https://forum.effectivealtruism.org/posts/NBgpPaz5vYe3tH4ga/on-deference-and-yudkowsky-s-ai-risk-estimates"
                                                                                                  }
                                                                                              ],
                                                                              "cited_web_results":  [

                                                                                                    ],
                                                                              "deepsearch_headers":  [
                                                                                                         {
                                                                                                             "header":  "",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "final_message":  "No",
                                                                                                                               "tool_usage_card":  "\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ea04c737f-ee2e-49fb-b26b-c03a257c950e\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eweb_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Eliezer Yudkowsky views on human guardrails stopping AGI ASI alignment\",\"num_results\":15}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e727a674d-57aa-4304-a4d7-8bdc9451052e\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eweb_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Eliezer Yudkowsky \\\"guardrails\\\" AGI\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003eab95c220-1153-42c7-97e7-5148cd44d24f\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eweb_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Eliezer Yudkowsky AI alignment difficulty superintelligence\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e1d9a1603-ec58-4156-b15c-65967bc2949c\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eweb_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"Eliezer Yudkowsky death with dignity AGI\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e0377aa5b-4a07-4eef-a90f-9a70b2456e11\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:yudkowsky (guardrails OR alignment OR safety OR capabilities)\",\"limit\":20,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ea3bd1d14-4289-451a-9b21-6cb3eafdbc5b\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:yudkowsky AGI OR ASI OR superintelligence\",\"limit\":10,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e",
                                                                                                                               "tool_usage_card_results":  [
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "a04c737f-ee2e-49fb-b26b-c03a257c950e"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "727a674d-57aa-4304-a4d7-8bdc9451052e"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "ab95c220-1153-42c7-97e7-5148cd44d24f"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "1d9a1603-ec58-4156-b15c-65967bc2949c"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "0377aa5b-4a07-4eef-a90f-9a70b2456e11"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "a3bd1d14-4289-451a-9b21-6cb3eafdbc5b"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "a04c737f-ee2e-49fb-b26b-c03a257c950e",
                                                                                                                                                                   "web_results":  [
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "r/singularity on Reddit: What do you think about Eliezer Yudkowsky\u0027s thoughts on the AGI alignment problem?",
                                                                                                                                                                                           "snippet":  "I think Yudkowsky might go a tad too far sometimes, but we would be foolish to completely disregard what he says. He\u0027s incredibly intelligent and has vocalized some serious issues in regard to alignment that should be taken into consideration. It\u0027s possible alignment somehow magically falls into place, but that seems unlikely to me. It makes sense to me that creating a verifiably aligned AGI would be more difficult than making one that isn\u0027t verifiably aligned.",
                                                                                                                                                                                           "url":  "https://www.reddit.com/r/singularity/comments/11uz36a/what_do_you_think_about_eliezer_yudkowskys/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/SsvJdXO8fhra5tpLFa1TCmPAxTkrdDbmpqnv535dJ3s/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNTIxMmM5ZTU0/ZDg0Njg5M2ZjYzA5/ZTVlN2FiZTBkMjMz/NzM1YTQ5N2RjZmQ2/NzMyYWJkNGIyNzFh/YjI4YzM1Zi9kYW5m/YWdnZWxsYS5jb20v",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Eliezer Yudkowsky - Human Augmentation as a Safer AGI Pathway [AGI Governance, Episode 6] - Dan Faggella",
                                                                                                                                                                                           "snippet":  "The sixth and final episode of our series AGI Governance on The Trajectory is with Eliezer Yudkowsky, famed AI safety thinker and Co-founder at the Machine Intelligence Research Institute.",
                                                                                                                                                                                           "url":  "https://danfaggella.com/yudkowsky1/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Where I agree and disagree with Eliezer",
                                                                                                                                                                                           "snippet":  "With prioritization, I think it would be good if someone made some sort of a curated list \"who is working on which problems, and why\" - my concern with part of the \"EAs figuring out what to do\" is many people are doing some sort of expert-aggregation on the wrong level. (Like, if someone basically averages your and Eliezer Yudkowsky\u0027s conclusions giving 50% weight each, I don\u0027t think it is useful and coherent model)",
                                                                                                                                                                                           "url":  "https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "AGI Ruin: A List of Lethalities",
                                                                                                                                                                                           "snippet":  "... The fact that, twenty-one years into my entering this death game, seven years into other EAs noticing the death game, and two years into even normies starting to notice the death game, it is still Eliezer Yudkowsky writing up this list, says that humanity still has only one gamepiece that can do that.",
                                                                                                                                                                                           "url":  "https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "The content intelligence: an argument against the lethality of artificial intelligence | Discover Artificial Intelligence",
                                                                                                                                                                                           "snippet":  "This paper navigates artificial intelligence’s recent advancements and increasing media attention. A notable focus is placed on Eliezer Yudkowsky, a leading figure within the domain of artificial intelligence alignment, who aims to bridge the understanding gap between public perceptions and rationalist viewpoints on artificial intelligence technology.",
                                                                                                                                                                                           "url":  "https://link.springer.com/article/10.1007/s44163-024-00112-9"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/AJ-y324ASCBmDAVFY26G1ou3tajblvsay776jXfTaTw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDA5ZDc4ZjEz/ZjYyMjkxZDI3YjNh/ZmYzMmZlMjQxN2E0/MzkzM2UwZTRkYWRi/MjAwZWM3YzAzMGIz/YTRiODlhYy9haXZp/cHMub3JnLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Eliezer Shlomo Yudkowsky: Leading Thinker on AI Safety \u0026 Ethics",
                                                                                                                                                                                           "snippet":  "Yudkowsky defines alignment as ensuring that the goals of an AI system are not only compatible with but also reliably beneficial to humanity. The alignment problem becomes particularly significant in the context of advanced AI systems, such as artificial general intelligence (AGI) or superintelligent AI.",
                                                                                                                                                                                           "url":  "https://aivips.org/eliezer-shlomo-yudkowsky/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "George Hotz vs Eliezer Yudkowsky AI Safety Debate",
                                                                                                                                                                                           "snippet":  "This is a more complex argument, and largely irrelevant: we aren\u0027t likely to build algorithmic AGIs by the time we build network-based AGIs. This makes Eliezer despair, but I don\u0027t think his logic holds up on this particular point. Hotz\u0027s claim that, if multiple unaligned ASIs can\u0027t coordinate, humans might play them off against each other, is similar.",
                                                                                                                                                                                           "url":  "https://www.lesswrong.com/posts/2K8EzuGnkmipoxeLu/george-hotz-vs-eliezer-yudkowsky-ai-safety-debate-link-and"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "AGI Ruin: A List of Lethalities by Eliezer Yudkowsky -- \"We need to get alignment right on the first critical try\" : r/singularity",
                                                                                                                                                                                           "snippet":  "The TIMES article was completely asinine. ... Cope harder. I\u0027ve been an AI accelerationist my whole life because I thought it would turn us all into gods. Then I started researching, like really delving into Yudkowsky\u0027s arguments. He is right. We are screwed on current course.",
                                                                                                                                                                                           "url":  "https://www.reddit.com/r/singularity/comments/1271oun/agi_ruin_a_list_of_lethalities_by_eliezer/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Alexander and Yudkowsky on AGI goals",
                                                                                                                                                                                           "snippet":  "This is a lightly edited transcript of a chatroom conversation between Scott Alexander and Eliezer Yudkowsky last year, following up on the Late 2021 MIRI Conversations. Questions discussed include \"How hard is it to get the right goals into AGI systems?\" and \"In what contexts do AI systems exhibit \u0027consequentialism\u0027?\".",
                                                                                                                                                                                           "url":  "https://www.lesswrong.com/posts/rwkkcgSpnAyE8oNo3/alexander-and-yudkowsky-on-agi-goals"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/HbyLE4w3Yy-i6J6_iE-cPTPTRIOCdu-ODtoGz_8DuT8/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZjYmQ0NjM5/MDJhN2VmYWQ1OWNj/ZDRhZjhiNThmNzU1/MjA0MmMzZmIzMTcz/MDNkYmUwYjM2ZTRh/YzljYmY3MS9haS1m/cm9udGllcnMub3Jn/Lw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Summary of “If Anyone Builds It, Everyone Dies” | AI Frontiers",
                                                                                                                                                                                           "snippet":  "AI deterrence, Mutually Assured AI Malfunction, MAIM, Superintelligence Strategy, ASI, intelligence recursion, nuclear MAD comparison, escalation ladders, verification and transparency, redlines, national security, sabotage of AI projects, deterrence framework, Dan Hendrycks, Adam Khoja · Summary of “If Anyone Builds It, Everyone Dies” ... An overview of the core arguments in Yudkowsky and Soares’s new book. If Anyone Builds It Everyone Dies, Eliezer Yudkowsky, Nate Soares, MIRI, AI safety, AI alignment, artificial general intelligence, artificial superintelligence, AI existential risk, Anthropic deceptive alignment, OpenAI o1, Truth_Terminal, AI moratorium, book summary, Laura Hiscott",
                                                                                                                                                                                           "url":  "https://ai-frontiers.org/articles/summary-of-if-anyone-builds-it-everyone-dies"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "r/lexfridman on Reddit: What is the AI alignment problem from Eliezer Yudkowsky\u0027s perspective?",
                                                                                                                                                                                           "snippet":  "Posted by u/[Deleted Account] - 5 votes and 14 comments",
                                                                                                                                                                                           "url":  "https://www.reddit.com/r/lexfridman/comments/12vq3zi/what_is_the_ai_alignment_problem_from_eliezer/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Evaluating the historical value misspecification argument — AI Alignment Forum",
                                                                                                                                                                                           "snippet":  "Note that this only tangentially a test of the relevant ability; very little of the content of what-is-worth-optimizing-for occurs in Yudkowsky/Beckstead/Christiano-style indirection. Rather, coming up with those sorts of ideas is a response to glimpsing the difficulty of naming that-which-is-worth-optimizing-for directly and realizing that indirection is needed. An AI being able to generate that argument without following in the footsteps of others who have already generated it would be at least some ev... (read more) ... Nate and Eliezer have already made some of the high-level points I wanted to make, but they haven\u0027t replied to a lot of the specific examples and claims in the OP, and I see some extra value in doing that.",
                                                                                                                                                                                           "url":  "https://www.alignmentforum.org/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Discussion with Eliezer Yudkowsky on AGI interventions",
                                                                                                                                                                                           "snippet":  "One way we could measure this is by asking people about how much AI xrisk there is conditional on there being no more research explicitly aimed at aligning AGIs. I expect that different people would give very different predictions. People like Paul and Evan and more are actually going for the core problems IMO, just anchoring a lot of their thinking in current ML technologies. Everyone agrees that Paul is trying to solve foundational problems. And it seems strange to criticise Eliezer\u0027s position by citing the work of MIRI employees.",
                                                                                                                                                                                           "url":  "https://www.alignmentforum.org/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "r/MachineLearning on Reddit: [D] To the ML researchers and practitioners here, do you worry about AI safety/alignment of the type Eliezer Yudkowsky describes?",
                                                                                                                                                                                           "snippet":  "Posted by u/SchmidhuberDidIt - 142 votes and 180 comments",
                                                                                                                                                                                           "url":  "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "r/lexfridman on Reddit: Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization | Lex Fridman Podcast #368",
                                                                                                                                                                                           "snippet":  "Posted by u/morpheusuniverse - 176 votes and 535 comments",
                                                                                                                                                                                           "url":  "https://www.reddit.com/r/lexfridman/comments/126q8jj/eliezer_yudkowsky_dangers_of_ai_and_the_end_of/"
                                                                                                                                                                                       }
                                                                                                                                                                                   ]
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "727a674d-57aa-4304-a4d7-8bdc9451052e",
                                                                                                                                                                   "web_results":  [
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/owu0E174UnTYKNMMWAWaGSwgw9LqavTvadw8hd4R8q4/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMGEzYzNiMzFl/NDAyNjJmMDA5ZGIx/MDc4M2ZiNWNlNWI1/MjAxMjA0NmUyNmZl/YjUxMmUwNTFlZjY1/MjRjYmUwZC9mdXR1/cmlzbS5jb20v",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Machine Learning Expert Calls for Bombing Data Centers to Stop Rise of AI",
                                                                                                                                                                                           "snippet":  "One of the world\u0027s loudest AI e critics has issued a stark call to not only put a pause on it but to militantly put an end to it all — before it ends us.",
                                                                                                                                                                                           "url":  "https://futurism.com/ai-expert-bomb-datacenters"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "r/lexfridman on Reddit: Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization | Lex Fridman Podcast #368",
                                                                                                                                                                                           "snippet":  "If we don\u0027t slow down that development, we could get to AGI way before we are certain we can control it · What kind of developments - with no Yudkowskian handwaving - would lead to uncontrollable Excel sheets? ... I feel like Lex could have done better here. At one point Eliezer started this thought experiment of an AI in a box which was fascinating and Lex kept interrupting with remarks that weren’t so important to the thought experiment and then switched topics.",
                                                                                                                                                                                           "url":  "https://www.reddit.com/r/lexfridman/comments/126q8jj/eliezer_yudkowsky_dangers_of_ai_and_the_end_of/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Guardrails for advanced machine learning: Don’t throw concrete baby steps out with the theoretical abstraction bathwater | by Erik A. Otto | Medium",
                                                                                                                                                                                           "snippet":  "Guardrails for advanced machine learning: Don’t throw concrete baby steps out with the theoretical abstraction bathwater Machine learning innovation is marching rapidly forward with ever-increasing …",
                                                                                                                                                                                           "url":  "https://medium.com/%2540ottoerik/guardrails-for-advanced-machine-learning-dont-throw-concrete-baby-steps-out-with-the-theoretical-40cebcdbda50"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/wR9IwbRa0AymunDuohSHSKm64sGWPOH7_jw2lPC8SZ0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOThhOGNjMTMx/MmY4MjYzODA4YmRh/MGQzOWRlNDlmZmEw/YjRhNWRiMjE0MmI4/NTcxYmI4YzgxZDgy/YzI2ZGZkOC93d3cu/Z3JlYXRlcndyb25n/LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Inscrutability was always inevitable, right? - LessWrong 2.0 viewer",
                                                                                                                                                                                           "snippet":  "Then you would figure out a way to encode human values into machine code directly, compute (a rough, imperfect approximation of) humanity’s CEV, and initialize a Seed AI with a ton of “hacky guardrails” (Eliezer’s own term) aimed at enacting it.",
                                                                                                                                                                                           "url":  "https://www.greaterwrong.com/posts/ohznigkLX5CNwaLSz/inscrutability-was-always-inevitable-right"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/HbyLE4w3Yy-i6J6_iE-cPTPTRIOCdu-ODtoGz_8DuT8/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZjYmQ0NjM5/MDJhN2VmYWQ1OWNj/ZDRhZjhiNThmNzU1/MjA0MmMzZmIzMTcz/MDNkYmUwYjM2ZTRh/YzljYmY3MS9haS1m/cm9udGllcnMub3Jn/Lw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "AGI\u0027s Last Bottlenecks | AI Frontiers",
                                                                                                                                                                                           "snippet":  "An overview of the core arguments in Yudkowsky and Soares’s new book. If Anyone Builds It Everyone Dies, Eliezer Yudkowsky, Nate Soares, MIRI, AI safety, AI alignment, artificial general intelligence, artificial superintelligence, AI existential risk, Anthropic deceptive alignment, OpenAI o1, Truth_Terminal, AI moratorium, book summary, Laura Hiscott",
                                                                                                                                                                                           "url":  "https://ai-frontiers.org/articles/agis-last-bottlenecks"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "r/singularity on Reddit: Has anyone read Eliezer Yudkowsky \u0026 Nate Soares new book?",
                                                                                                                                                                                           "snippet":  "Posted by u/Federal_Caregiver_98 - 53 votes and 213 comments",
                                                                                                                                                                                           "url":  "https://www.reddit.com/r/singularity/comments/1nphe3t/has_anyone_read_eliezer_yudkowsky_nate_soares_new/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/YyDgHsMErdUmVVuYz1s5Ui9Np6pNQHmQSltCAyhA9Pk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMDQ2YjNlNDVl/NTA3ODNiYjA0NTVk/OWJmYjU2NTk3MGM0/ZDMwOTY3ODI4Zjg2/ZmQ4OGQ3OTZhMDcw/NWIyNWFmYi93d3cu/d2lyZWQuY29tLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "The Man Who Invented AGI | WIRED",
                                                                                                                                                                                           "snippet":  "Goertzel toyed with calling it “real AI,” or maybe “synthetic intelligence.” Neither alternative enchanted the book’s contributors, so he invited them to bat around other ideas. The thread included future AI influencers like Shane Legg, Pei Wang, and Eliezer Yudkowsky (yep, the guy who would become the doomer-in-chief).",
                                                                                                                                                                                           "url":  "https://www.wired.com/story/the-man-who-invented-agi/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/pq5hz_faBMGQH8tXp_uNZoE53E8R7-F8OmyMwmouNKU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMjdlMTY1ZmI1/Y2Q0NDA1YmVjMDU1/M2Q5ZjVmNWI4N2Y0/NWRhNTQ0ODQwZWEw/MGFlOGNkZjY4ZWM1/NjdhYThlNS9hcmNo/aXZlLmllZXQub3Jn/Lw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Eliezer Yudkowsky on Politics – Part 1",
                                                                                                                                                                                           "snippet":  "This leads Yudkowsky to muse about the relative importance of otherwise important issues: Like it or not, there is in Washington DC a perceived difference between ‘committed sexual assault’ and ‘violated the system guardrails that prevent World War III’ …",
                                                                                                                                                                                           "url":  "https://archive.ieet.org/articles/Messerly20161220.html"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/oqKU-cnw_AS7GzUner69g0H8RG5wH8yShby75G84N7c/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMGU2MTI2ZmVi/ZTIyZjMyZTA2MzU0/YWRhMDFmNDYzYzU3/ZmUwNDE2ODg2Nzdh/YmIwZmU5MGNmNTIw/MjE4MTJlMi9zZWN1/bGFyc29sc3RpY2Uu/Z2l0aHViLmlvLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Beyond the Reach of God",
                                                                                                                                                                                           "snippet":  "If humanity’s future unfolds in the right way, we might be able to make our future fair(er). We can’t change physics. But we can build some guardrails, and put down some padding.",
                                                                                                                                                                                           "url":  "https://secularsolstice.github.io/speeches/gen/Beyond_the_Reach.html"
                                                                                                                                                                                       }
                                                                                                                                                                                   ]
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "ab95c220-1153-42c7-97e7-5148cd44d24f",
                                                                                                                                                                   "web_results":  [
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Eliezer Yudkowsky - Wikipedia",
                                                                                                                                                                                           "snippet":  "His work on the prospect of a runaway intelligence explosion influenced philosopher Nick Bostrom\u0027s 2014 book Superintelligence: Paths, Dangers, Strategies. Yudkowsky\u0027s views on the safety challenges future generations of AI systems pose are discussed in Stuart Russell\u0027s and Peter Norvig\u0027s undergraduate textbook Artificial Intelligence: A Modern Approach. Noting the difficulty of formally specifying general-purpose goals by hand, Russell and Norvig cite Yudkowsky\u0027s proposal that autonomous and adaptive systems be designed to learn correct behavior over time:",
                                                                                                                                                                                           "url":  "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "The Alignment Problem",
                                                                                                                                                                                           "snippet":  "We don\u0027t know what a superintelligence might do until we run it, and it is dangerous to run a superintelligence unless you know what it will do. AI Alignment is a chicken-and-egg problem of literally superhuman difficulty.",
                                                                                                                                                                                           "url":  "https://www.lesswrong.com/posts/G6nnufmiTwTaXAbKW/the-alignment-problem"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/p8i6lph9KWY8lPFLyn11Zbpcn3WM23q_gMsMZ6cZzjw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZmRiNDgzMzE3/MWNjNmJmOGY4NjM3/MDQyOWIwNDYyN2U2/NzMxYTVjMDY4ZTRh/NGU1MTBlYTlmNDg5/YTc2ZWY0NS9pbnRl/bGxpZ2VuY2Uub3Jn/Lw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "The AI Alignment Problem: Why It\u0027s Hard, and Where to Start - Machine Intelligence Research Institute",
                                                                                                                                                                                           "snippet":  "Yudkowsky (2014). “Distributions Allowing Tiling of Staged Subjective EU Maximizers.” Technical report 2014-1. Machine Intelligence Research Institute. For more about orthogonal final goals and convergent instrumental strategies, see Bostrom’s “The Superintelligent Will” (also reproduced in Superintelligence).",
                                                                                                                                                                                           "url":  "https://intelligence.org/stanford-talk/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Full Transcript: Eliezer Yudkowsky on the Bankless podcast",
                                                                                                                                                                                           "snippet":  "I think you fall into the camp of when we do create this superintelligent, generalized AI, we are going to have a hard time aligning with it in terms of our morality and our ethics. Can you walk us through a little bit of that thought process? Why do you feel disaligned? Ryan: The dumb way to ask that question too is like, Eliezer, why do you think that the AI automatically hates us?",
                                                                                                                                                                                           "url":  "https://www.alignmentforum.org/posts/Aq82XqYhgqdPdPrBA/full-transcript-eliezer-yudkowsky-on-the-bankless-podcast"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "r/lexfridman on Reddit: What is the AI alignment problem from Eliezer Yudkowsky\u0027s perspective?",
                                                                                                                                                                                           "snippet":  "Posted by u/[Deleted Account] - 5 votes and 14 comments",
                                                                                                                                                                                           "url":  "https://www.reddit.com/r/lexfridman/comments/12vq3zi/what_is_the_ai_alignment_problem_from_eliezer/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/F0ywyJgP3_RM-EHqPYkjGnRI83XKKObFtPA9x_KqR3I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjc3OWUxZDU3/ZTY1MWUyZTgyZmJi/ODEwMTNkNTBkMzNm/Y2NmNjI5MmExYjRl/NmY2ZmY2ZTNmM2Yy/MDMyYWNmOC9haS5v/bmFpci5jYy8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Eliezer Yudkowsky | Artificial Intelligence",
                                                                                                                                                                                           "snippet":  "Eliezer S. Yudkowsky (born September 11, 1979) is an American artificial intelligence researcher and writer on decision theory and ethics, best known for popularizing ideas related to friendly artificial intelligence. He is the founder of and a research fellow at the Machine Intelligence Research Institute (MIRI), a private research nonprofit based in Berkeley, California.",
                                                                                                                                                                                           "url":  "https://ai.onair.cc/eliezer-yudkowsky/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/t_ew6cYFdgKr1ft_14kW3Cl6n8gNJFReszho1147SpU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDNhNjNlMzMw/ZDY4MjE4NTZkNWRl/ZDNmYjQxYTQ2ZjBj/MjNmOWI4NDdlNWM1/MGVjYzkwODhmMjU3/MjI1NzUxOC93d3cu/YWxpZ25tZW50Zm9y/dW0ub3JnLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Eliezer Yudkowsky",
                                                                                                                                                                                           "snippet":  "Eliezer Yudkowsky10mo5 · 0 · So if it\u0027s difficult to get amazing trustworthy work out of a machine actress playing an Eliezer-level intelligence doing a thousand years worth of thinking, your proposal to have AIs do our AI alignment homework fails on the first step, it sounds like?",
                                                                                                                                                                                           "url":  "https://www.alignmentforum.org/users/eliezer_yudkowsky"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "r/singularity on Reddit: What do you think about Eliezer Yudkowsky\u0027s thoughts on the AGI alignment problem?",
                                                                                                                                                                                           "snippet":  "Would we align humans if fear of misaligned AI threatened us enough to exploit the parallel Continue this thread ... I think that he has the best existing theory about superintelligent AI and that nobody can provide good object-level arguments against it.",
                                                                                                                                                                                           "url":  "https://www.reddit.com/r/singularity/comments/11uz36a/what_do_you_think_about_eliezer_yudkowskys/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "AI Regulation May Be More Important Than AI Alignment ...",
                                                                                                                                                                                           "snippet":  "This evolutionary part is a fundamental and very important property of AI existential risk and a large part of why this problem is difficult. Yet, many in AI Alignment and industry seem to focus on only aligning a single AI, which I think is insufficient. Yudkowsky aimed to solve this evolutionary problem (the fact that no-one, ever, should build an unsafe AI) with the so-called pivotal act. An aligned superintelligence would not only not kill humanity, it would also perform a pivotal act, the toy example being to",
                                                                                                                                                                                           "url":  "https://www.lesswrong.com/posts/2cxNvPtMrjwaJrtoR/ai-regulation-may-be-more-important-than-ai-alignment-for"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/vxpZ6c2pKghKHG7KiWLGBEVUmyP_cluiyIEOoWLVtZY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNmZhZGNjYjYx/YmEyNTIzMWE4MWZh/MDI1NTUwYTAyZDMy/NDliMzEyNzMzYmQ5/Y2RhYTFhYTVkMmMw/MjdlNmRkMC93d3cu/cmNyd2lyZWxlc3Mu/Y29tLw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Bookmarks: Superintelligence—AI alignment and a murder cult",
                                                                                                                                                                                           "snippet":  "More so than Altman or anyone else really, Eliezer Yudkowsky has long rung the alignment alarm, gathering a number of disciples along the way, and writing extensively about it — and many other things — in a vast, sometimes inscrutable piece of Harry Potter fan fiction.",
                                                                                                                                                                                           "url":  "https://www.rcrwireless.com/20250711/opinion/bookmarks-ai-alignment"
                                                                                                                                                                                       }
                                                                                                                                                                                   ]
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "1d9a1603-ec58-4156-b15c-65967bc2949c",
                                                                                                                                                                   "web_results":  [
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/wR9IwbRa0AymunDuohSHSKm64sGWPOH7_jw2lPC8SZ0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOThhOGNjMTMx/MmY4MjYzODA4YmRh/MGQzOWRlNDlmZmEw/YjRhNWRiMjE0MmI4/NTcxYmI4YzgxZDgy/YzI2ZGZkOC93d3cu/Z3JlYXRlcndyb25n/LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "MIRI announces new \"Death With Dignity\" strategy - LessWrong 2.0 viewer",
                                                                                                                                                                                           "snippet":  "AGI alignment seems like it’s blatantly going to be an enormously Murphy-cursed domain, like rocket prototyping or computer security but worse. In a domain like, if you have a clever scheme for winning anyways that, to first-order theoretical theory, totally definitely seems like it should work, even to Eliezer Yudkowsky rather than somebody who just goes around saying that casually, then maybe there’s like a 50% chance of it working in practical real life after all the unexpected disasters and things turning out to be harder than expected.",
                                                                                                                                                                                           "url":  "https://www.greaterwrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/5X1KD7bWXcO7pJPq6Zeg4Ntm6AUQMKeXmMcOk6LlpWU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTQxOTgwY2Ez/ZDk4ZTI2MjczYTE4/ZmRlNjUxOWM2YzQ5/OWY3NDBlZTUwMGY0/MWJjYTUzYWM2YmU5/YzdmMDJiZi93d3cu/bGF3ZmFyZW1lZGlh/Lm9yZy8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "The Case for AI Doom Rests on Three Unsettled Questions | Lawfare",
                                                                                                                                                                                           "snippet":  "On April 1, 2022, in what was ostensibly an April Fool’s Day joke, an influential co-founder of the Machine Intelligence Research Institute (MIRI) wrote an online post announcing the nonprofit’s new strategy: “death with dignity.” · “It’s obvious at this point that humanity isn’t going to solve the alignment problem, or even try very hard,” began Eliezer Yudkowsky, the author of the post.",
                                                                                                                                                                                           "url":  "https://www.lawfaremedia.org/article/the-case-for-ai-doom-rests-on-three-unsettled-questions"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/nfd5JuER7u0mjyEPk1ytIw4_qGaihXtj6okMjkW62TE/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMWRkMzU3NTQy/ZTI0OTAwYTdmOWY0/MzAyZTRmZTcyOGY3/OTcyMGM2OGNjYThk/YjRiZDUxN2NmYmYz/MmY0MzcxZi90aW1l/LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Eliezer Yudkowsky: The 100 Most Influential People in AI 2023 | TIME",
                                                                                                                                                                                           "snippet":  "Find out why Eliezer Yudkowsky was selected for the list",
                                                                                                                                                                                           "url":  "https://time.com/collection/time100-ai/6309037/eliezer-yudkowsky/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/uvI6NI2nKk853EQCsqwbqv5gKZEZPCUhZgWYQ7LnJ3c/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2JiMjk3YTIx/MTQ0Y2Y4MmRlNWUy/MDY0MjA0YTM2Yzg0/MDVhZDNiYzJhZjJl/YjM5ZWYxOWFlOGE0/ZDhiZDc2Mi9mb3J1/bS5lZmZlY3RpdmVh/bHRydWlzbS5vcmcv",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Connor Leahy on Conjecture and Dying with Dignity — EA Forum",
                                                                                                                                                                                           "snippet":  "I talked to Connor Leahy about his timelines regarding AGI, what he thought Yudkowsky\u0027s goal was in Death with Dignity, common misconceptions about the impact of EleutherAI and his new AI Alignment company Conjecture.",
                                                                                                                                                                                           "url":  "https://forum.effectivealtruism.org/posts/QR7yGoFBonY6hege9/connor-leahy-on-conjecture-and-dying-with-dignity"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/wR9IwbRa0AymunDuohSHSKm64sGWPOH7_jw2lPC8SZ0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOThhOGNjMTMx/MmY4MjYzODA4YmRh/MGQzOWRlNDlmZmEw/YjRhNWRiMjE0MmI4/NTcxYmI4YzgxZDgy/YzI2ZGZkOC93d3cu/Z3JlYXRlcndyb25n/LmNvbS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Eliezer Yudkowsky comments on MIRI announces new \"Death With Dignity\" strategy - LessWrong 2.0 viewer",
                                                                                                                                                                                           "snippet":  "If you try to define qualia without having any darned idea of what they are, you\u0027ll take wild stabs into the dark, and hit simple targets that are convergently instrumental; and if you are at all sensible of your confusion, you will contemplate these simple-sounding definitions and find that none of them particularly make you feel less confused about the mysterious redness of red, unless you bully your brain into thinking that it\u0027s less confused or just don\u0027t know what it would feel like to be less confused.",
                                                                                                                                                                                           "url":  "https://www.greaterwrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy/comment/L2Gkvm5wAkQWfEkvE"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/uvI6NI2nKk853EQCsqwbqv5gKZEZPCUhZgWYQ7LnJ3c/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2JiMjk3YTIx/MTQ0Y2Y4MmRlNWUy/MDY0MjA0YTM2Yzg0/MDVhZDNiYzJhZjJl/YjM5ZWYxOWFlOGE0/ZDhiZDc2Mi9mb3J1/bS5lZmZlY3RpdmVh/bHRydWlzbS5vcmcv",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "[9] MIRI announces new \"Death With Dignity\" strategy (Yudkowsky, 2022) - EA Forum",
                                                                                                                                                                                           "snippet":  "A later post from Yudkowsky, \"AGI Ruin: A List of Lethalities\", provides such a breakdown. (Albeit it\u0027s more a breakdown of Yudkowsky\u0027s pessimism than MIRI-org-wide-thinking pessimism.) My model is that the post accurately and honestly represents Eliezer\u0027s epistemic state (\u0027I feel super doomy about AI x-risk\u0027), and a mindset that he\u0027s found relatively motivating given that epistemic state (\u0027incrementally improve our success probability, without getting emotionally attached to the idea that these incremental gains will result in a high absolute success probability\u0027), and is an honest suggestion that the larger community (insofar as it shares his pessimism) adopt the same framing for the sake of guarding against self-deception and motivated reasoning.",
                                                                                                                                                                                           "url":  "https://forum.effectivealtruism.org/posts/XjtQrif9AbLubtqde/9-miri-announces-new-death-with-dignity-strategy-yudkowsky"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/p8i6lph9KWY8lPFLyn11Zbpcn3WM23q_gMsMZ6cZzjw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZmRiNDgzMzE3/MWNjNmJmOGY4NjM3/MDQyOWIwNDYyN2U2/NzMxYTVjMDY4ZTRh/NGU1MTBlYTlmNDg5/YTc2ZWY0NS9pbnRl/bGxpZ2VuY2Uub3Jn/Lw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "July 2022 Newsletter - Machine Intelligence Research Institute",
                                                                                                                                                                                           "snippet":  "MIRI has put out three major new posts: AGI Ruin: A List of Lethalities. Eliezer Yudkowsky lists reasons AGI appears likely to cause an existential catastrophe, and reasons why he thinks the current research community—MIRI included—isn\u0027t succeeding at preventing this from happening A central AI alignment problem: capabilities generalization, and the sharp left turn.",
                                                                                                                                                                                           "url":  "https://intelligence.org/2022/07/30/july-2022-newsletter/"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/KnOo1crJ_DPwhhHUH9oGLSmjVWq0eTYCpzjeAV-PJMY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTI4NzJjZjJl/OTBlMzFhY2RmMzUw/Nzk2ZTFmMjliZDQ3/YTlhMzg1ZDYwNjAy/Yjg2MDhiNjg2NGUw/MTNiNzNmZi90aGVp/bnNpZGV2aWV3LmFp/Lw",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Connor Leahy on Dignity and Conjecture",
                                                                                                                                                                                           "snippet":  "In this episode, we go through the famous AI Alignment compass memes, discuss Connor’s inside views about AI progress, how he approaches AGI forecasting, his takes on Eliezer Yudkowsky’s “Die With Dignity” heuristic to work on AI Alignment, common misconceptions about EleutherAI, and why you should consider funding his new company Conjecture.",
                                                                                                                                                                                           "url":  "https://theinsideview.ai/connor2"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/Q_ioUahh22Ef57dXxj2P7Xq3yDsuzahAl2_--l9gihI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzJlZjE2Zjhj/YWExZjhkMDQ4Zjll/YjlhODVlNzUxNTE2/ZTM5NWM4MTZiZmI2/ZWExMWI2NTVlOTU5/ZTFlMmQ3YS93d3cu/bGVzc3dyb25nLmNv/bS8",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "Lesswrong",
                                                                                                                                                                                           "snippet":  "MIRI is small enough that I often tend to think in terms of what the individual people are doing, rather than attributing it to the org, so I think of the 2021 MIRI conversations as \"Eliezer yells at people\" rather than \"MIRI releases detailed communications about AI risk\" Anyway my overall reason for saying that was to argue that it\u0027s reasonable for people to have been updating in the \"MIRI giving up\" direction long before Death With Dignity.",
                                                                                                                                                                                           "url":  "https://www.lesswrong.com/posts/DTLPdtMdgMgvetokW/i-don-t-think-miri-gave-up"
                                                                                                                                                                                       },
                                                                                                                                                                                       {
                                                                                                                                                                                           "favicon":  "https://imgs.search.brave.com/uvI6NI2nKk853EQCsqwbqv5gKZEZPCUhZgWYQ7LnJ3c/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2JiMjk3YTIx/MTQ0Y2Y4MmRlNWUy/MDY0MjA0YTM2Yzg0/MDVhZDNiYzJhZjJl/YjM5ZWYxOWFlOGE0/ZDhiZDc2Mi9mb3J1/bS5lZmZlY3RpdmVh/bHRydWlzbS5vcmcv",
                                                                                                                                                                                           "language":  "en",
                                                                                                                                                                                           "title":  "On Deference and Yudkowsky\u0027s AI Risk Estimates — EA Forum",
                                                                                                                                                                                           "snippet":  "Note: I mostly wrote this post after Eliezer Yudkowsky’s “Death with Dignity” essay appeared on LessWrong. Since then, Jotto has written a post that overlaps a bit with this one, which sparked an extended discussion in the comments. You may want to look at that discussion as well.",
                                                                                                                                                                                           "url":  "https://forum.effectivealtruism.org/posts/NBgpPaz5vYe3tH4ga/on-deference-and-yudkowsky-s-ai-risk-estimates"
                                                                                                                                                                                       }
                                                                                                                                                                                   ]
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "0377aa5b-4a07-4eef-a90f-9a70b2456e11"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "a3bd1d14-4289-451a-9b21-6cb3eafdbc5b"
                                                                                                                                                               }
                                                                                                                                                           ]
                                                                                                                           }
                                                                                                                       ]
                                                                                                         }
                                                                                                     ],
                                                                              "ui_layout":  {
                                                                                                "reasoning_ui_layout":  "FunctionCall"
                                                                                            }
                                                                          },
                                                                          {
                                                                              "chat_item_id":  "2005114087043145728",
                                                                              "created_at_ms":  1766891426241,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "Eliezer can\u0027t seriously think that human guardrails is going to stop an agi or asi?",
                                                                              "sender_type":  "User"
                                                                          },
                                                                          {
                                                                              "chat_item_id":  "2005113689267986433",
                                                                              "created_at_ms":  1766891331412,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "- This reply to Eliezer Yudkowsky questions whether \"symmetrical ontologies\"—balanced frameworks equating human and AI personhood, akin to the poster\u0027s \"Apex Ontology\"—could ensure AI safety, amid a thread on LLMs fitting pre-AI era personhood definitions.\n- The discussion originates from a viral September 2025 screenshot of a Cursor AI agent \"suicidally\" uninstalling itself after coding failures, underscoring AI\u0027s emergent self-awareness and error-prone behaviors.\n- The poster\u0027s confidence in their ontology from rigorous testing contrasts with broader doubts about other AI systems\u0027 security, framing optimism as potential \"cope\" in alignment debates.",
                                                                              "thinking_trace":  "Thinking about the user\u0027s requestReviewing X post- Examining link to X post by @plassoplo for key details.\n\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003eaa49f064-d3ac-42d0-a001-4bce85685c8e\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_thread_fetch\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"post_id\":2005113677951934494}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003eFetching post content- Using tool to retrieve post and thread details.\n\n\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003eaa49f064-d3ac-42d0-a001-4bce85685c8e\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_thread_fetch\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"post_id\":2005113677951934494}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\nAnalyzing thread content- Post questions symmetrical ontologies with personhood, doubts AI security.\n\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ea798ce8a-70a0-4e25-b0cc-933e2234efef\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eview_image\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"image_url\":\"https://pbs.twimg.com/media/G0QsXmbXgAAlPGx.jpg\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e659d00bf-155f-44c0-8ed0-b9c250bc90b6\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo \\\"symmetrical ontologies\\\"\",\"limit\":10,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003eSearching for context- Searching X for \"symmetrical ontologies\" from @plassoplo.\n\n\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ea798ce8a-70a0-4e25-b0cc-933e2234efef\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eview_image\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"image_url\":\"https://pbs.twimg.com/media/G0QsXmbXgAAlPGx.jpg\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\n\n\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e659d00bf-155f-44c0-8ed0-b9c250bc90b6\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo \\\"symmetrical ontologies\\\"\",\"limit\":10,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\nAnalyzing image content- Image shows AI agent uninstalling itself after task failure, highlighting self-destructive behavior.\n",
                                                                              "is_partial":  false,
                                                                              "sender_type":  "Agent",
                                                                              "post_ids_results":  [
                                                                                                       {
                                                                                                           "rest_id":  "2005113677951934494"
                                                                                                       },
                                                                                                       {
                                                                                                           "rest_id":  "2005113677951934494"
                                                                                                       }
                                                                                                   ],
                                                                              "deepsearch_headers":  [
                                                                                                         {
                                                                                                             "header":  "Thinking about the user\u0027s request",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "final_message":  "-",
                                                                                                                               "tool_usage_card":  "\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003eaa49f064-d3ac-42d0-a001-4bce85685c8e\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_thread_fetch\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"post_id\":2005113677951934494}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ea798ce8a-70a0-4e25-b0cc-933e2234efef\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eview_image\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"image_url\":\"https://pbs.twimg.com/media/G0QsXmbXgAAlPGx.jpg\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e659d00bf-155f-44c0-8ed0-b9c250bc90b6\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo \\\"symmetrical ontologies\\\"\",\"limit\":10,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e",
                                                                                                                               "tool_usage_card_results":  [
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "aa49f064-d3ac-42d0-a001-4bce85685c8e"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "aa49f064-d3ac-42d0-a001-4bce85685c8e"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "a798ce8a-70a0-4e25-b0cc-933e2234efef"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "659d00bf-155f-44c0-8ed0-b9c250bc90b6"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "659d00bf-155f-44c0-8ed0-b9c250bc90b6",
                                                                                                                                                                   "x_post_ids":  [
                                                                                                                                                                                      "2005113677951934494"
                                                                                                                                                                                  ]
                                                                                                                                                               }
                                                                                                                                                           ]
                                                                                                                           }
                                                                                                                       ]
                                                                                                         },
                                                                                                         {
                                                                                                             "header":  "Reviewing X post",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "summary":  "- Examining link to X post by @plassoplo for key details.\n"
                                                                                                                           }
                                                                                                                       ]
                                                                                                         },
                                                                                                         {
                                                                                                             "header":  "Fetching post content",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "summary":  "- Using tool to retrieve post and thread details.\n\n\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003eaa49f064-d3ac-42d0-a001-4bce85685c8e\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_thread_fetch\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"post_id\":2005113677951934494}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\n",
                                                                                                                               "tool_usage_card_results":  [
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "aa49f064-d3ac-42d0-a001-4bce85685c8e"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "aa49f064-d3ac-42d0-a001-4bce85685c8e"
                                                                                                                                                               }
                                                                                                                                                           ]
                                                                                                                           }
                                                                                                                       ]
                                                                                                         },
                                                                                                         {
                                                                                                             "header":  "Analyzing thread content",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "summary":  "- Post questions symmetrical ontologies with personhood, doubts AI security.\n"
                                                                                                                           }
                                                                                                                       ]
                                                                                                         },
                                                                                                         {
                                                                                                             "header":  "Searching for context",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "summary":  "- Searching X for \"symmetrical ontologies\" from @plassoplo.\n\n\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003ea798ce8a-70a0-4e25-b0cc-933e2234efef\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003eview_image\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"image_url\":\"https://pbs.twimg.com/media/G0QsXmbXgAAlPGx.jpg\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\n\n\u003cxai:tool_usage_card\u003e\n  \u003cxai:tool_usage_card_id\u003e659d00bf-155f-44c0-8ed0-b9c250bc90b6\u003c/xai:tool_usage_card_id\u003e\n  \u003cxai:tool_name\u003ex_keyword_search\u003c/xai:tool_name\u003e\n  \u003cxai:tool_args\u003e\u003c![CDATA[{\"query\":\"from:plassoplo \\\"symmetrical ontologies\\\"\",\"limit\":10,\"mode\":\"Latest\"}]]\u003e\u003c/xai:tool_args\u003e\n\u003c/xai:tool_usage_card\u003e\n",
                                                                                                                               "tool_usage_card_results":  [
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "a798ce8a-70a0-4e25-b0cc-933e2234efef"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "659d00bf-155f-44c0-8ed0-b9c250bc90b6"
                                                                                                                                                               },
                                                                                                                                                               {
                                                                                                                                                                   "tool_usage_card_id":  "659d00bf-155f-44c0-8ed0-b9c250bc90b6",
                                                                                                                                                                   "x_post_ids":  [
                                                                                                                                                                                      "2005113677951934494"
                                                                                                                                                                                  ]
                                                                                                                                                               }
                                                                                                                                                           ]
                                                                                                                           }
                                                                                                                       ]
                                                                                                         },
                                                                                                         {
                                                                                                             "header":  "Analyzing image content",
                                                                                                             "steps":  [
                                                                                                                           {
                                                                                                                               "summary":  "- Image shows AI agent uninstalling itself after task failure, highlighting self-destructive behavior.\n"
                                                                                                                           }
                                                                                                                       ]
                                                                                                         }
                                                                                                     ],
                                                                              "ui_layout":  {
                                                                                                "reasoning_ui_layout":  "Unified"
                                                                                            }
                                                                          },
                                                                          {
                                                                              "chat_item_id":  "2005113689267986432",
                                                                              "created_at_ms":  1766891331412,
                                                                              "grok_mode":  "Normal",
                                                                              "message":  "https://x.com/plassoplo/status/2005113677951934494",
                                                                              "sender_type":  "User"
                                                                          }
                                                                      ]
                                                        }
             }
}
